<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title></title>
    <link>http://localhost:4000</link>
    <description>
      some mathematics &amp; computation
    </description>
    
        
            <item>
                <title>Navigating a convex body online</title>
                <link>http://localhost:4000/online/2018/04/06/navigating/</link>
                <content:encoded>
                    <![CDATA[
                    <p>In the last lecture, we saw some algorithms that, while simple and appealing,
were somewhat unmotivated.  We now try to derive them from general
principles, and in a setting that will allow us to attack other
problems in competitive analysis.
<script type="math/tex">\def\K{\mathsf{K}}
\def\R{\mathbb{R}}
\def\seteq{\mathrel{\vcenter{:}}=}
\def\cE{\mathcal{E}}
\def\argmin{\mathrm{argmin}}
\def\llangle{\left\langle}
\def\rrangle{\right\rangle}
\def\1{\mathbf{1}}
\def\e{\varepsilon}</script></p>

<h2 id="gradient-descent--the-proximal-view">Gradient descent:  The proximal view</h2>

<p>Let us first recall the upper bound we derived for the regret in the last lecture:</p>

<p>
\begin{equation}\label{eq:regret}
   R_T \leq \sum_{t=1}^T \left[ \|p_t-p_{t+1}\|_1 + \left\langle p_{t+1}, \ell_t - \ell_t(x_T^*) \right\rangle\right].
\end{equation}
</p>

<p>Trying to minimize this expression this leads to the question of how we should update our probability distribution $p_t \to p_{t+1}$
to simultaneously be stable (control the first term) and competitive (the second term).</p>

<p>A very natural algorithm in this setting is gradient descent.
Indeed, suppose that $\ell : \R^n \to \R$ is differentiable, and consider the optimization</p>
<p>
\[
   \min \left\{  \frac12 \|x-x_0\|^2 + \eta \ell(x) : x \in \R^n \right\},
\]
</p>
<p>where $\eta &gt; 0$ is a small constant and $\|\cdot\|$ denotes the Euclidean norm.  Then first-order optimality
conditions dictate that the optimizer satisfies
[
   x^* = x_0 - \eta \nabla \ell(x_0) + O(\eta^2)\,.
]</p>

<p>Two questions immediately arise:  Why do we use the Euclidean norm when our reference problem \eqref{eq:regret}
refers to the $\ell_1$ norm, and if $x$ is meant to encode a probability distribution,
how do we maintain this constraint for $x^*$?</p>

<h2 id="projected-gradient-descent">Projected gradient descent</h2>

<p>Let’s address the feasibility problem first.  Suppose $\K \subseteq \R^n$ is a closed convex set
and $F : \R^n \to \R^n$ is a sufficiently smooth vector field (think of $F = \nabla \ell$).
How should we move in the direction of $F$ while simultaneously remaining inside $\K$?</p>

<p>The unconstrained flow along $F$ can be described as
a trajectory $x : [0,\infty) \to \R^n$ given by
[
   x’(t) = F(x(t))\,.
]
The most natural way to keep this flow inside $\K$ is to project back into the body whenever we leave.
Define the Euclidean projection</p>
<p>
\[
   P_{\K}(y) \seteq \argmin \left\{ \|y-z\|^2 : z \in \K \right\},
\]
</p>
<p>and the result of taking an infinitesimal step in direction $v$ and and then projecting:
[
   \Pi_{\K}(x,v) \seteq \lim_{\e \to 0} \frac{P_{\K}(x+\e v) -x}{\e}\,.
]
Then the projected dynamics looks like
[
   x’(t) = \Pi_{\K} \left(x(t), F(x(t))\right)\,.
]
(As we will see, this is well-defined under mild assumptions.)
This maintains feasibility,
but still leaves us with the question of what
role the Euclidean norm is playing.</p>

<h2 id="a-riemannian-version">A Riemannian version</h2>

<p>One can view $\Pi_{\K}(x, \cdot)$ as a function on the tangent space at $x$.
To specify such a projection,
we only need a local Euclidean structure.
An inner product $\langle \cdot,\cdot\rangle_x$ that varies smoothly
over $x \in \R^n$ is precisely a Riemannian metric.</p>

<p>Equivalently, we specify at every point $x \in \R^n$, a smoothly varying positive-definite matrix $M(x)$
so that</p>
<p>
\begin{align*}
   \langle u,v\rangle_{M,x} &amp;= \langle u, M(x) v\rangle \\
   \|u\|^2_{M,x} &amp;= \langle u,u\rangle_{M,x}.
\end{align*}
</p>
<p>The associated projection operator is then given by</p>
<p>
\begin{align*}
   P_{\K}^M(y; x) &amp;\seteq \argmin \left\{ \left\|y-z\right\|_{M,x}^2 : z \in \K \right\} \\
   \Pi_{\K}^M(x,v) &amp;\seteq \lim_{\e \to 0} \frac{P_{\K}^M(x+\e v,x)-x}{\e}\,.
\end{align*}
</p>
<p>This leads to the dynamical system:</p>
<p>
\begin{align*}
   x'(t) &amp;= \Pi^M_{\K}\left(x(t),F(x(t))\right) \\
   x(0) &amp;= x_0 \in \K\,.
\end{align*}
</p>

<h2 id="lyapunov-functions">Lyapunov functions</h2>

<p>The problem with stating things at this level of generality is that even when
$F = \nabla \ell$ is the gradient of a convex function $\ell : \R^n \to \R$,
we don’t have a global way of controlling convergence of $F(x(t))$ to $\min \{ F(x) : x \in \K \}$.
In the Euclidean setting ($M(x) \equiv \mathrm{Id}$), there is a natural <a href="https://en.wikipedia.org/wiki/Lyapunov_function">Lyapunov function</a>:
If $\ell$ is convex and $\ell(x^*) = \min \{ \ell(x) : x \in \K \}$,
then for every $x \in \K$:</p>
<p>
\[
   \langle - \nabla \ell(x), x^* - x\rangle \geq 0\,.
\]
</p>
<p>In other words,
gradient descent always makes progress toward $x^*$.</p>

<p>If $x’(t) = \Pi_{\K}\left(x(t), \nabla \ell(x(t))\right)$, then
in the language of competitive analysis, the quantity $\frac12 \|x(t)-x^*\|^2$
acts a potential function (a global measure of progress).</p>

<p>We will consider geometries that come equipped with such a
Lyapunov function.  In a sense that can be formalized in various ways,
these are the <em>Hessian structures on $\R^n$</em>, i.e., those
arising when $M(x) = \nabla^2 \Phi(x)$ for some strongly convex function $\Phi : \K \to \R$.</p>

<h1 id="mirror-descent-dynamics">Mirror descent dynamics</h1>

<p>Consider now a compact, convex set $\K \subseteq \R^n$,
a strongly convex function $\Phi : \K \to \R$,
and a continuous time-varying vector field $F : [0,\infty) \times \K \to \R^n$.
We will refer to <strong>continuous-time mirror descent</strong>
as the dynamics specified by</p>
<p>
\begin{align*}
   x'(t) &amp;= \Pi_{\K}^{\nabla^2 \Phi}\left(\vphantom{\bigoplus} x(t), F(t, x(t))\right) \\
   x(0)  &amp;= x_0 \in \K.
\end{align*}
</p>

<p>As one might expect, we can decompose $x’(t)$ into two components:  One flowing in the direction $F(t,x(t))$,
and the other component arising from the normal forces that are keeping $x(t)$ inside $\K$.
We recall the <strong>normal cone to $\K$ at $x$</strong> is given by</p>
<p>
\[
   N_{\K}(x) = \left\{ p \in \R^n : \langle p,y -x \rangle \leq 0 \textrm{ for all } y \in \K \right\}.
\]
</p>
<p>This is the set of directions that point out of the body $\K$.
The next theorem is proved in 
the paper
<a href="https://homes.cs.washington.edu/~jrl/papers/pdf/kserver.pdf">k-server via multiscale entropic regularization</a>.</p>

<p class="theorem" text="MD">
<a name="thm:md"></a>
   If $\nabla^2 \Phi(x)^{-1}$ is continuous on $\K$, then for any $x_0 \in \K$, there
   is an absolutely continuous trajectory $x : [0,\infty) \to \K$ satisfying
   \begin{align*}
      \nabla^2 \Phi(x(t)) x'(t) &amp;\in F(t,x(t)) - N_{\K}(x(t)), \\
      x(0) &amp;= x_0.
   \end{align*}
   Moreover, if $\nabla^2 \Phi(x)$ is Lipschitz on $\K$ and $F$ is locally Lipschitz, then the solution is unique.
</p>

<h2 id="lagrangian-multipliers">Lagrangian multipliers</h2>

<p>If $\K$ is a polyhedron, the one can write</p>
<p>
\begin{equation}\label{eq:polyhedron}
   \K = \{ x \in \R^n : Ax \leq b \}, \qquad A \in \R^{m \times n}, b \in \R^m\,.
\end{equation}
</p>
<p>In this case, the normal cone at $x$ is the cone spanned by the normals of the tight constraints at $x$:</p>
<p>
\begin{equation}\label{eq:cone-poly}
   N_{\K}(x) = \left\{ A^T y : y \geq 0 \textrm{ and } y^T(b-Ax)=0 \right\}.
\end{equation}
</p>

<p>Consider now the application of <a href="#thm:md">Theorem MD</a> to a polyhedron and a solution $x : [0,\infty) \to \K$,
$\lambda : [0,\infty) \to \R^n$ such that</p>
<p>
\begin{equation}\label{eq:traj}
   \nabla^2 \Phi(x(t)) x'(t) = F(t,x(t)) - \lambda(t),
\end{equation}
</p>
<p>and $\lambda(t) \in N_{\K}(x(t))$ for $t \geq 0$.</p>

<p>Let us consider the dual variables to the constraints \eqref{eq:polyhedron}:
We can fix a measurable $\hat{\lambda} : [0,\infty) \to \R^m_+$ such that
[
   A^T \hat{\lambda}(t) = \lambda(t), \quad t \geq 0.
]
Now \eqref{eq:cone-poly} and $\lambda(t) \in N_{\K}(x(t))$ yield the complementary-slackness
conditions:  For all $i=1,2,\ldots,m$ and $t \geq 0$:
[
   \hat{\lambda}_i(t) &gt; 0 \implies \langle A_i,x(t)\rangle = b_i,
]
where $A_i$ is the $i$th row of $A$.</p>

<h2 id="the-bregman-divergence-as-a-lyapunov-function">The Bregman divergence as a Lyapunov function</h2>

<p>We promised earlier the existence of a functional to control the dynamics,
and this is provided by the <strong>Bregman divergence associated to $\Phi$</strong>:</p>
<p>
\[
   D_{\Phi}(y; x) \seteq \Phi(y) - \Phi(x) - \langle \nabla \Phi(x), y-x\rangle\,.
\]
</p>
<p>Let $x(t)$ be a trajectory satisfying \eqref{eq:traj}.  Then for any $y \in \K$:</p>
<p>
\begin{align}
   \partial_t D_{\Phi}(y; x(t)) &amp;= - \langle \nabla \Phi(x(t)), x'(t)\rangle  + \langle \nabla \Phi(x(t)), x'(t) \rangle
   -\langle \partial_t \Phi(x(t)), y-x(t)\rangle \nonumber \\
                                &amp;= - \langle \nabla^2 \Phi(x(t)) x'(t), y - x(t) \rangle \nonumber \\
                                &amp;= - \langle F(t,x(t)) - \lambda(t), y-x(t)\rangle \nonumber \\
                                &amp;\leq - \langle F(t,x(t)), y-x(t)\rangle \label{eq:div}\,,
\end{align}
</p>
<p>where the last inequality used that $y \in \K$ and $\lambda(t) \in N_{\K}(x(t))$.</p>

<p>If $F(t,x(t))=c(t)$ is a <em>cost function</em>, say, then this inequality
aligns with a goal stated at the beginning of the first lecture:
As long as the algorithm $x(t)$ is suffering more cost than some feasible point $y \in \K$,
we would like to be “learning” about $y$.</p>

<h2 id="the-algorithm-from-last-time">The algorithm from last time</h2>

<p>In the next lecture, we will use this framework to derive and analyze
algorithms for metrical task systems (MTS) and the $k$-server problem.
For now, let us show that the algorithm and analysis
from last time (for MTS on uniform metrics) fit precisely into our framework.</p>

<p>Suppose that $\K = \{ x \in \R_+^n : \sum_{i=1}^n x_i = 1 \}$ is the probability simplex and</p>
<p>
\[
   \Phi(x) = \sum_{i=1}^n (x_i+\delta) \log (x_i+\delta)
\]
</p>
<p>is the (negative) entropy with some shift by $\delta &gt; 0$.</p>

<p>Then $\nabla^2 \Phi(x)$ is a diagonal matrix with $\left(\nabla^2 \Phi(x)\right)_{ii} = \frac{1}{x_i+\delta}$.
Let $F(t,\cdot) = -c(t)$ be a time-varying cost vector with $c(t) \geq 0$.</p>

<p>Therefore \eqref{eq:traj} gives</p>
<p>
\begin{equation}\label{eq:shifted}
   x_i'(t) = (x_i(t)+\delta) \left(-c_i(t) + \hat{\mu}(t) - \hat{\lambda}_i(t)\right).
\end{equation}
Here, $\hat{\lambda}_i(t)$ is the Lagrangian multiplier corresponding to the constraint $x_i \geq 0$,
and $\hat{\mu}(t)$ is the multiplier corresponding to $\sum_{i=1}^n x_i = 1$.
</p>

<p>This is precisley the algorithm described before (as an exercise,
one might try rewriting it to match exactly), and \eqref{eq:div} constitutes half of the analysis.
In the next lecture, we will discuss some general methods for the other half:  Tracking the movement cost.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/online/2018/04/06/navigating/</guid>
                <description>
                    
                    Continuous-time mirror descent analysis
                    
                </description>
                <pubDate>Fri, 06 Apr 2018 00:00:00 -0700</pubDate>
                <author>James R. Lee</author>
            </item>
        
    
        
            <item>
                <title>Regret minimization and competitive analysis</title>
                <link>http://localhost:4000/online/2018/04/01/competitive-analysis/</link>
                <content:encoded>
                    <![CDATA[
                    <p>These are notes for the first lecture of a course I am co-teaching with <a href="https://sbubeck.com/">Seb Bubeck</a> on
<a href="https://homes.cs.washington.edu/~jrl/teaching/cse599I-spring-2018/">Competitive analysis via convex optimization</a>.</p>

<p>I want to set the groundwork by reviewing the bandits model in online learning
and the standard exponential weights algorithm, and then trying to extend it
to the setting of competitive analysis where the analogous
problem goes by the name of <em>metrical task systems</em>.
Seeing where things go wrong will give us a plan to follow for much of the course.
Some of the objects and algorithms in this lecture may appear
unmotivated; that will be addressed soon.</p>

<h2 id="regret-minimization">Regret minimization</h2>

<p>Here we review quickly the basic <em>multi-arm bandits model</em> in online learning.
For more background see, e.g., this <a href="http://sbubeck.com/SurveyBCB12.pdf">survey of Bubeck and Cesa-Binachi</a>.
<script type="math/tex">\def\seteq{\mathrel{\vcenter{:}}=}
\def\cE{\mathcal{E}}
\def\R{\mathbb{R}}
\def\argmin{\mathrm{argmin}}
\def\llangle{\left\langle}
\def\rrangle{\right\rangle}
\def\1{\mathbf{1}}
\def\e{\varepsilon}</script></p>

<p>In the standard bandits model, we have a set of
experts $\cE = \{1,2,\ldots,N\}$, and
loss functions
$\ell_1,\ell_2,\ldots$ arriving over time,
where $\ell_t : \cE \to [0,1]$.</p>

<p>For the sake of simplicity, we will work in the full
information model.  At time $t \geq 1$, we have seen $\ell_1,\ldots,\ell_{t-1}$.
We choose some strategy $x_t \in \cE$ and incur cost $\ell_t(x_t)$.
Our goal is to minimize the total cost incurred:
$\sum_{t=1}^T \ell_t(x_t)$.
In the setting of adversarial bandits, we will allow ourselves
to employ a randomized strategy, playing instead a distribution $p_t : \cE \to [0,1]$ at time $t$.</p>

<p>In the regret minimization framework, we compare our expected loss to
that of the optimal fixed strategy:  The <em>regret incurred up to time $T$</em> is</p>
<p>
$$R_T \seteq \sum_{t=1}^T \langle p_t, \ell_t\rangle - \min_{x \in \cE} \sum_{t=1}^T \ell_t(x)\,,$$
</p>
<p>where for $f,g : \cE \to \R$, we write $\langle f,g\rangle = \sum_{y \in \cE} f(y) g(y)$.</p>

<p>We will bound the regret in two steps.
Denote $x_T^* \seteq \argmin_{x \in \cE} \sum_{t=1}^T \ell_t(x)$.  Then:</p>
<p>
\begin{align}
   R_T &amp;= \sum_{t=1}^T \langle p_t - p_{t+1},\ell_t\rangle + 
   \sum_{t=1}^T \langle p_{t+1}, \ell_t-\ell_t(x_T^*)\rangle \nonumber \\
   &amp;\leq \sum_{t=1}^T \|p_t-p_{t+1}\|_1 + 
   \sum_{t=1}^T \langle p_{t+1}, \ell_t-\ell_t(x_T^*)\rangle,\label{eq:pseudo-regret}
\end{align}
</p>
<p>where the inequality uses that the losses lie in $[0,1]$.</p>

<h3 id="continuous-time-dynamics">Continuous-time dynamics</h3>

<p>For a number of reasons, the analysis will be much cleaner in continuous time.
We can think about the losses as a trajectory $\left\{ \ell_t : \cE \to [0,1] \mid t \geq 0 \right\}$,
where the instantaneous loss incurred is $\ell_t\,dt$.
As long as we are bounding the expression \eqref{eq:pseudo-regret},
our continuous-time model will be <em>more general</em>
than the discrete-time model.  The latter can be recovered by
considering a trajectory $\{ \ell_t : t \geq 0 \}$
that is piecewise-constant.
If we were bounding the regret itself, the continuous-time model would have a time advantage,
but once we shift time by one as in \eqref{eq:pseudo-regret},
this advantage disappears.</p>

<p>Now we can analogously bound the regret:</p>
<p>
\begin{equation}\label{eq:cont-regret}
   R_T \leq \int_0^T \|\partial_t p_t\|_1 \,dt + \int_0^T \langle p_t, \ell_t-\ell_t(x_T^*)\rangle\,dt\,,
\end{equation}
</p>
<p>where $x_T^* \seteq \argmin_{x \in \cE} \int_0^T \ell_t(x)\,dt$.</p>

<p>We will start with $p_0(x) = 1/N$ for every $x \in \cE$, and
employ the following exponential-weights strategy for updating $p_t$:
\begin{equation}\label{eq:dynamics}
\partial_t \log p_t(x) = \eta\left( - \ell_t(x) + \langle p_t, \ell_t\rangle\right),
\end{equation}
where $\eta &gt; 0$ is a parameter called the <em>learning rate</em> that we will choose soon.
This algorithm will be motivated more in the next lecture, but for now one can note that
we are simply doing continuous-time gradient descent on the vector $\log p_t(x)$:  We are moving
in the direction $- \eta \ell_t dt$.  The additional additive term in \eqref{eq:dynamics}
is there to maintain the constraint that $p_t$ is a probability distribution.</p>

<p>One can see this clearly by rewriting \eqref{eq:dynamics} as:
\begin{equation}\label{eq:exp-form}
   \partial_t p_t(x) = p_t(x) \, \eta \left(- \ell_t(x) + \langle p_t,\ell_t\rangle\right),
\end{equation}
so that $\sum_{x \in \cE} \partial_t p_t(x) = 0$.</p>

<h3 id="the-lyapunov-functional-aka-the-potential">The Lyapunov functional, aka the potential</h3>

<p>In order to bound the regret $R_T$, we will employ the philosophy underlying
the entire course:  If we are incurring more cost than $x_T^*$, we would like to be <em>learning</em>
about $x_T^*$ in a suitable sense.</p>

<p>Define:
[
   D(x; p) \seteq - \log p(x)\,.
]
Note that for any $x \in \cE$, we have $D(x; p_0) = \log N$, and $D(x; p_t) \geq 0$ for all $t \geq 0$.</p>

<p>For any $x \in \cE$:
\begin{equation}\label{eq:lya}
   \partial_t D(x;p_t) = - \partial_t \log p_t(x) = \eta \left(\ell_t(x) - \langle p_t,\ell_t\rangle \right).
\end{equation}
If we think of $D(x;p_t)$ as the “distance” from $p_t$ to $x$, then our distance to $x$
is decreasing proportional to the advantage $x$ has over our strategy $p_t$.
Integrating \eqref{eq:lya} over $[0,T]$ gives
[
   D(x; p_0) - D(x; p_T) = \eta \int_0^T \langle p_t, \ell_t-\ell_t(x)\rangle\,dt
]</p>

<p>Applying this with $x=x_T^*$ and recalling \eqref{eq:cont-regret}, we have</p>
<p>
\begin{align*}
   R_T  &amp;\leq \int_0^T \|\partial_t p_t\|_1\,dt + \frac{D(x_0^*;p_0)-D(x_T^*;p_T)}{\eta} \\
        &amp;\leq \int_0^T \|\partial_t p_t\|_1\,dt + \frac{\log N}{\eta} \\
        &amp;\leq \eta T + \frac{\log N}{\eta}\,,
\end{align*}
</p>
<p>where the last inequality uses \eqref{eq:exp-form} and the fact that the losses are in $[0,1]$
to write $\|\partial_t p_t\|_1 \leq \eta \|p_t\|_1 = \eta$.</p>

<p>Setting $\eta = \sqrt{\frac{\log N}{T}}$ yields the standard regret bound
[
   R_T \leq 2 \sqrt{T \log N}\,.
]
If one does not know the final time $T$, it is not difficult to see that one can choose
a time-dependent learning rate $\eta=\eta(t)=\sqrt{\frac{\log N}{t}}$ to obtain a similar result.
result</p>

<h2 id="competitive-analysis">Competitive analysis</h2>

<p>The competitive analysis analog of the bandits framework goes by the name of
<em>metrical task systems (MTS)</em>.  This problem was introduced in 1992 by
<a href="http://www.cs.huji.ac.il/~nati/PAPERS/bls_online.pdf">Borodin, Linial, and Saks</a>.</p>

<p>This setting has three major differences:</p>
<ul>
  <li>At each time $t \geq 1$, we receive a <em>cost function</em> $c_t : \cE \to [0,\infty)$, and we choose
an action $x_t \in \cE$ <em>after</em> receiving $c_t$.</li>
  <li>There is a metric $d$ on $\cE$ that makes $(\cE,d)$ into a metric space,
and in addition to the <em>service cost</em> $c_t(x_t)$, we pay a <em>switching cost</em> $d(x_{t-1},x_t)$
for playing $x_t$.</li>
  <li>We compare ourselves against an offline optimum that has the <em>same ability to switch strategies</em>.</li>
</ul>

<p>Say that an online algorithm $\llangle x_1, x_2, \ldots,x_T\rrangle$ is $\alpha$-competitive
if, for every $x_0 \in \cE$ and every cost sequence $c_1,c_2,\ldots,c_T$, it holds that</p>
<p>
$$
   \sum_{t=1}^T c_t(x_t) + d(x_{t-1},x_t) \leq \alpha\left(\sum_{t=1}^T c_t(x^*_t) + d(x_{t-1}^*, x_t^*)\right) + O(1)\,,
$$
</p>
<p>where $\llangle x_0^*, x_1^*, x_2^*, \ldots, x_T^*\rrangle$ is the optimal <em>offline</em> strategy
with $x_0^*=x_0$, i.e., the optimal strategy in hindsight.  The additive $O(1)$ term is a constant
that should be independent of the request sequence.</p>

<p>It is conjectured that the competitive ratio is $O(\log N)$ for every $N$-point metric space.
In the coming weeks, we will see an $O((\log N)^2)$-competitive algorithm
based on upcoming joint work with Bubeck, Cohen, and Y. T. Lee.
This improves slightly on the $O((\log N)^2 \log \log N)$ bound of <a href="https://arxiv.org/abs/cs/0406034">Fiat and Mendel</a>.</p>

<h3 id="attempting-exponential-weights">Attempting exponential weights</h3>

<p>The simplest setting for MTS is when the metric $(\cE,d)$ is uniform, i.e., $d(x,y)=\1_{\{x\neq y\}}$ for $x,y \in \cE$.</p>

<p>Just as in the bandits setting, we can consider a continuous-time trajectory
$\left\{ c_t : \cE \to [0,\infty) \mid t \geq 0\right\}$ on cost
functions.  And we might try to employ a similar strategy:
\begin{equation}\label{eq:cost-dynamics}
   \partial_t \log p_t(x) = - c_t(x) + \langle p_t, c_t\rangle.
\end{equation}</p>

<p>But now we run into a major hurdle:  Such an algorithm cannot be $\alpha$-competitive for any $\alpha &lt; \infty$.</p>

<p>
Suppose we arrange that for some $x_0 \in \cE$ and $t_0 &gt; 0$ and small $\epsilon &gt; 0$,
it holds that $p_{t_0}(x_0) = 1-\epsilon$.
We can do this by having an adversary play the cost function $c(x)=\1_{\cE \setminus \{x_0\}}(x)$
for a long enough period of time
so that any competitive algorithm must put almost all the probability mass on $x_0$.
</p>

<p>
Now the adversary changes the cost function to $c(x)=\1_{x_0}(x)$.
The dynamics specified by \eqref{eq:cost-dynamics} will move much too slowly!
Indeed:
\[
   \partial_t \log p_t(x_0) = -1+(1-p_t(x_0)) = - p_t(x_0),
\]
i.e.,
\[
   \partial_t p_t(x_0) = - p_t(x_0)^2,
\]
thus it will take roughly $1/\epsilon$ time before $p_t(x_0) &lt; 1/2$.
</p>

<p>Thus our algorithm will incur cost $\asymp 1/\epsilon$ while the optimal offline algorithm incurs cost $O(1)$.
In the bandits setting, this was fine because every fixed strategy incurs cost $\asymp 1/\epsilon$.
But in the setting of competitive analysis, our algorithm needs to be a lot more nimble
to keep up with an offline algorithm that can switch strategies.</p>

<h3 id="the-exploration-shift">The exploration shift</h3>

<p>To fix this, we will design an algorithm that devotes a constant fraction of the service cost
it is currently incurring to exploring the strategy space.
Essentially, this can be achieved by pretending that $p_t(x) \geq 1/(2N)$ for every $x \in \cE$.
(Recall that $N = |\cE|$.)
This transformation (mixing with the uniform distribution)
is not uncommon in the bandit literature.
In the setting of metrical task systems, I saw it for the first time
in this paper of <a href="/assets/papers/pot.pdf">Bansal, Buchbinder, and Naor</a> on the weighted paging problem.</p>

<p>Define $p_0(x)=\1_{x_0}(x)$ where $x_0 \in X$ is the starting point.
Let $\delta &gt; 0$ be a number we will choose soon,
and consider the dynamics:
\begin{equation}\label{eq:mts-dynamics}
   \partial_t \log (p_t(x)+\delta) = - \hat{c}_t(x) + \llangle \frac{p_t+\delta}{1+\delta N}, \hat{c}_t\rrangle\,,
\end{equation}
which can be written equivalently as
\begin{equation}\label{eq:mts-move}
   \partial_t p_t(x) = (p_t(x)+\delta) \left(- \hat{c}_t(x) + 
   \llangle \frac{p_t+\delta}{1+\delta N}, \hat{c}_t\rrangle\right).
\end{equation}
A natural choice is to take $\hat{c}_t(x)=c_t(x)$, but this presents a problem:
Unlike \eqref{eq:cost-dynamics}, these dynamics no longer enforce
that $p_t(x) \geq 0$.</p>

<h4 id="lagrangian-multipliers">Lagrangian multipliers</h4>

<p>In this relatively simple setting,
we can consider reduced costs
$\hat{c}_t(x) \seteq c_t(x)-\lambda_t(x)$ satisfying:</p>

<ul>
  <li>$\lambda_t(x) \geq 0$ for all $t \geq 0$,</li>
  <li>$p_t(x) = 0 \implies \partial_t p_t(x) \geq 0$,</li>
  <li>$\lambda_t(x) &gt; 0 \implies p_t(x)=0$.</li>
</ul>

<p>With these constraints in place,
the corresponding trajectory $\{p_t : t \geq 0\}$
will always be a probability measure,
and we can charge ourselves $\hat{c}_t(x)$ without
worrying about cheating, since $p_t(x) \hat{c}_t(x) = p_t(x) c_t(x)$
will always hold.
The existence of functions $\{ \lambda_t : \cE \to [0,\infty) \mid t \geq 0\}$
is a slightly subtle issue that will be addressed formally
in the coming lectures.  These are Lagrangian multipliers corresponding
to the constraints $p_t(x) \geq 0$ for $x \in \cE$.</p>

<p>As we will see in future lectures, the Lagrangian multipliers
will not adversely affect the potential analysis,
but they could cause our algorithm to incur movement cost.
In the present setting, things are going in the beneficial direction:
The multipliers correspond to <em>reduced</em> costs, and therefore
they actually slow down our movement.</p>

<h4 id="the-potential-analysis">The potential analysis</h4>

<p>
Define now the potential
\[
   D_{\delta}(x; p) \seteq - \log (p(x)+\delta).
\]
We are interested in $\partial_t D_{\delta}(x_t^*; p_t)$.
Let's first consider the derivative with respect to $x_t^*$.
For any $x,y \in \cE$:
\[
   \left|D_{\delta}(x; p) - D_{\delta}(y ;p)\right| \leq \log(1/\delta)\,.
\]
Thus for any $T \geq 0$:
\begin{equation}\label{eq:first-mts}
   D_{\delta}(x_{T}^*; p_T) - D_{\delta}(x_0^*, p_0)
   \leq \log(1/\delta) \sum_{t=1}^{\lfloor T\rfloor} d(x_t^*, x_{t-1}^*) + \int_0^T \partial_t D_{\delta}\left(x^*_{\lfloor t\rfloor}; p_t\right)\,dt\,.
\end{equation}
To analyze the latter term, use \eqref{eq:mts-dynamics}
to observe that for any $x \in \cE$,
\[
   \partial_t D_{\delta}(x;p_t) = \hat{c}_t(x) - \llangle \frac{p_t+\delta}{1+\delta N}, \hat{c}_t\rrangle,
\]
hence
\[
   \int_0^T \partial_t D_{\delta}\left(x_{\lfloor t\rfloor}^*; p_t\right)\,dt =
   \int_0^T \hat{c}_t(x_t^*)\,dt - \int_0^T \llangle \frac{p_t+\delta}{1+\delta N}, \hat{c}_t\rrangle\,dt\,.
\]
Plugging this into \eqref{eq:first-mts} and rearranging gives
\begin{align}\nonumber
   (1+&amp; \delta N)^{-1} \int_0^T \langle p_t + \delta, \hat{c}_t\rangle\,dt \\
&amp;\leq 
   \left[ D_{\delta}(x_{0}^*; p_0) - D_{\delta}(x_T^*, p_T) \right]
    + \log(1/\delta) \sum_{t=1}^{\lfloor T\rfloor} d(x_t^*, x_{t-1}^*) + 
   \int_0^T \hat{c}_t(x_t^*)\,dt  \nonumber \\
   &amp;\leq
    \log(1/\delta) \sum_{t=1}^{\lfloor T\rfloor} d(x_t^*, x_{t-1}^*) + 
    \int_0^T c_t(x_t^*)\,dt\,,\label{eq:second-mts}
\end{align}
where we have used the fact that the term in brackets is nonpositive,
and $c_t \geq \hat{c}_t$ pointwise.
This looks great:  We have
bounded the service cost of our algorithm by the movement and service
costs of the optimal algorithm.
We are left to consider the movement cost of the algorithm.
</p>

<h3 id="the-movement-cost">The movement cost</h3>

<p>
Here we use a trick from online algorithms:
Instead of bounding the total movement cost
$\int_0^T \|\partial_t p_t\|_1\,dt,$
we will bound only the <em>incoming movement</em>
$\int_0^T \|\left(\partial_t p_t\right)_+\|_1\,dt$.
</p>

<p>Since “what goes in must come out (unless it stays there forever),” we have:</p>
<p>
$$
   \int_0^T \|\partial_t p_t\|_1\,dt \leq
   2 \int_0^T \|\left(\partial_t p_t\right)_+\|_1\,dt + 1\,.
$$
And now \eqref{eq:mts-move} gives
$$
   \left\|\left(\partial_t p_t\right)_+\right\|_1 \leq \langle p_t+\delta,\hat{c}_t\rangle\,.
$$
</p>

<p>
Combining this with \eqref{eq:second-mts} and using the fact that $\langle p_t, c_t\rangle = \langle p_t, \hat{c}_t\rangle$
yields
\begin{align*}
   \int_0^T \left(\langle p_t,c_t\rangle + \|\partial_t p_t\|_1\right)\,dt &amp;\leq
   1 + 3 \int_0^T \langle p_t + \delta ,\hat{c}_t\rangle\,dt \\
   &amp; \leq 1 + 3(1+\delta N) \left[\log(1/\delta) \sum_{t=1}^{\lfloor T\rfloor} d(x_t^*, x_{t-1}^*) + 
\int_0^T c_t(x_t^*)\,dt\right].
\end{align*}
Thus setting $\delta = 1/N$ yields an $O(\log N)$-competitive algorithm for MTS on a uniform metric space.
</p>

<h4 id="a-comment-about-absolute-continuity">A comment about absolute continuity</h4>

<p>Note that in \eqref{eq:first-mts}, we have used the fundamental theorem of calculus
to integrate a derivative.
In order for this to be valid, it must be that $\log p_t(x)$ is absolutely continuous
as a function of $t$.
When we argue formally about the existence of the Lagrangian multipliers $\lambda_t$,
we will need to ensure that the resulting trajectory is absolutely continuous.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/online/2018/04/01/competitive-analysis/</guid>
                <description>
                    
                    Regret minimization vs. competitive analysis
                    
                </description>
                <pubDate>Sun, 01 Apr 2018 00:00:00 -0700</pubDate>
                <author>James R. Lee</author>
            </item>
        
    
        
            <item>
                <title>tcsmath relaunch</title>
                <link>http://localhost:4000/admin/2018/03/31/relaunch/</link>
                <content:encoded>
                    <![CDATA[
                    <p>I am relaunching tcsmath on a new platform in anticipation
of resuming regular posting.  The old pages
should still be available here <a href="https://tcsmath.wordpress.com/">tcsmath</a>,
and some of them will be slowly migrated to the new format.
There may be some DNS/https hiccups.  I hope those are resolved soon.</p>


                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/admin/2018/03/31/relaunch/</guid>
                <description>
                    
                    A relaunching of tcsmath
                    
                </description>
                <pubDate>Sat, 31 Mar 2018 00:00:00 -0700</pubDate>
                <author>James R. Lee</author>
            </item>
        
    
        
            <item>
                <title>An entropy optimal drift</title>
                <link>http://localhost:4000/entropy/2015/11/21/follmer-drift/</link>
                <content:encoded>
                    <![CDATA[
                    <h2 id="construction-of-föllmers-drift">Construction of Föllmer’s drift</h2>

<p>In a previous post, we saw how an entropy-optimal drift process
could be used to prove the Brascamp-Lieb inequalities.
Our main tool was a result of Föllmer that we now recall and justify.
Afterward, we will use it to prove the Gaussian log-Sobolev inequality.</p>

<p>Consider $f : \mathbb R^n \to \mathbb R_+$ with $\int f \,d\gamma_n = 1$,
where $\gamma_n$ is the standard Gaussian measure on $\mathbb R^n$.
Let $\{B_t\}$ denote an $n$-dimensional Brownian motion with $B_0=0$.
We consider all processes of the form
\begin{equation}\label{eq:drift}
W_t = B_t + \int_0^t v_s\,ds\,,
\end{equation}
where $\{v_s\}$ is a progressively measurable drift
and such that $W_1$ has law $f\,d\gamma_n$.</p>

<div class="theorem" text="Energy-entropy">
It holds that
\[
D(f d\gamma_n \,\|\, d\gamma_n) = \min D(W_{[0,1]} \,\|\, B_{[0,1]}) = \min \frac12 \int_0^1 \mathbb{E}\,\|v_t\|^2\,dt\,,
\]
where the minima are over all processes of the form \eqref{eq:drift}.
</div>

<div class="proof">
In the preceding post (Lemma 2), we have already seen that
for any drift of the form \eqref{eq:drift}, it holds that
\[
D(f d\gamma_n \,\|\,d\gamma_n) \leq \frac12 \int_0^1 \mathbb{E}\,\|v_t\|^2\,dt \leq D(W_{[0,1]} \,\|\, B_{[0,1]})\,,
\]
thus we need only exhibit a drift $\\{v_t\\}$ achieving equality.

<p>
We define
\[
v_t = \nabla \log P_{1-t} f(W_t) = \frac{\nabla P_{1-t} f(W_t)}{P_{1-t} f(W_t)}\,,
\]
where $\\{P_t\\}$ is the Brownian semigroup defined by
\[
P_t f(x) = \mathbb{E}[f(x + B_t)]\,.
\]
</p>

<p>
Note that $v_t$ is almost surely constant conditioned on the past, hence the chain rule yields
\begin{equation}\label{eq:chain}
D(W_{[0,1]} \,\|\, B_{[0,1]}) =
\frac12 \int_0^1 \mathbb{E}\,\|v_t\|^2\,dt\,.
\end{equation}
(See line (7) of Lemma 2 in the previous post.  Note that $h(v_t)=0$ since $v_t$ is deterministic given the past.)
We are left to show that $W_1$ has law $f \,d\gamma_n$ and $D(W_{[0,1]} \,\|\, B_{[0,1]}) = D(f d\gamma_n \,\|\,d\gamma_n)$.
</p>

<p>
We will prove the first fact using Girsanov's theorem to argue about
the change of measure between $\{W_t\}$ and $\{B_t\}$.
As in the previous post, we will argue somewhat informally
using the heuristic that the law of $dB_t$ is a Gaussian
random variable in $\mathbb R^n$ with covariance $dt \cdot I$.
It&ocirc;'s formula states that this heuristic is justified (see our use
of the formula below).
</p>

The following lemma says that, given any sample path $\{W_s : s \in [0,t]\}$
of our process up to time $s$, the probability that Brownian motion (without drift)
would have
"done the same thing is $\frac{1}{M_t}$.
</div>

<div class="remark">
I chose to present various steps in the next proof at varying levels of formality.
The arguments have the same structure as corresponding formal proofs,
but I thought (perhaps na&iuml;vely) that this would be instructive.
</div>

<div class="lemma" text="Heuristic Girsanov">
Let $\mu_t$ denote the law of $\\{W_s : s \in [0,t]\\}$.
If we define
\[
M_t = \exp\left(-\int_0^t \langle v_s,dB_s\rangle - \frac12 \int_0^t \|v_s\|^2\,ds\right)\,,
\]
then under the measure $\nu_t$ given by
\[
d\nu_t = M_t \,d\mu_t\,,
\]
the process $\\{W_s : s \in [0,t]\\}$ has the same law as $\\{B_s : s \in [0,t]\\}$.
</div>

<div class="proof">
We argue by analogy with the discrete proof.
First, let us define the infinitesimal ``transition kernel'' of Brownian motion
using our heuristic that $dB_t$ has covariance $dt \cdot I$:
\[
p(x,y) = \frac{e^{-\|x-y\|^2/2dt}}{(2\pi dt)^{n/2}}\,.
\]
<p>
We can also compute the (time-inhomogeneous) transition kernel $q_t$ of $\\{W_t\\}$:
\[
q_t(x,y) =  \frac{e^{-\|v_t dt + x - y\|^2/2dt}}{(2\pi dt)^{n/2}} = p(x,y) e^{-\frac12 \|v_t\|^2 dt} e^{-\langle v_t, x-y\rangle}\,.
\]
Here we are using that $dW_t = dB_t + v_t\,dt$ and $v_t$ is deterministic conditioned on the past, thus
the law of $dW_t$ is a normal with mean $v_t\,dt$ and covariance $dt \cdot I$.
</p>

<p>
To avoid confusion of derivatives, let's use $\alpha_t$ for the density of $\mu_t$ and $\beta_t$ for the density of
Brownian motion (recall that these are densities on paths).
Now let us relate the density $\alpha_{t+dt}$ to the density $\alpha_{t}$.
We use here the notations $\\{\hat W_t, \hat v_t, \hat B_t\\}$ to denote
a (non-random) sample path of $\\{W_t\\}$:
\begin{align*}
\alpha_{t+dt}(\hat W_{[0,t+dt]}) &amp;= \alpha_t(\hat W_{[0,t]})  q_t(\hat W_t, \hat W_{t+dt}) \\
&amp;=  \alpha_t(\hat W_{[0,t]}) p(\hat W_t, \hat W_{t+dt}) e^{-\frac12 \|\hat v_t\|^2\,dt-\langle \hat v_t,\hat W_t-\hat W_{t+dt}\rangle} \\
&amp;=
\alpha_t(\hat W_{[0,t]})  p(\hat W_t, \hat W_{t+dt}) e^{-\frac12 \|\hat v_t\|^2\,dt+\langle \hat v_t,d \hat W_t\rangle} \\
&amp;=
\alpha_t(\hat W_{[0,t]})  p(\hat W_t, \hat W_{t+dt}) e^{\frac12 \|\hat v_t\|^2\,dt+\langle \hat v_t, d \hat B_t\rangle}\,,
\end{align*}
where the last line uses $d\hat W_t = d\hat B_t + \hat v_t\,dt$.
</p>

Now by ``heuristic'' induction, we can assume $\alpha_t(\hat W_{[0,t]})=\frac{1}{M_t} \beta_t(\hat W_{[0,t]})$, yielding
\begin{align*}
\alpha_{t+dt}(\hat W_{[0,t+dt]}) &amp;= \frac{1}{M_t} \beta_t(\hat W_{[0,t]})  p(\hat W_t, \hat W_{t+dt}) e^{\frac12 \|\hat v_t\|^2\,dt+\langle \hat v_t, d \hat B_t\rangle} \\
&amp;=
\frac{1}{M_{t+dt}}  \beta_t(\hat W_{[0,t]}) p(\hat W_t, \hat W_{t+dt}) \\
&amp;=
\frac{1}{M_{t+dt}}  \beta_{t+dt}(\hat W_{[0,t+dt]})\,.
\end{align*}
In the last line, we used the fact that $p$ is the infinitesimal transition kernel for Brownian motion.
</div>

<h2 id="the-gaussian-log-sobolev-inequality">The Gaussian log-Sobolev inequality</h2>

<p>Consider again a measurable $f : \mathbb R^n \to \mathbb R_+$ with $\int f\,d\gamma_n=1$.
Let us define $\mathrm{Ent}_{\gamma_n}(f) = D(f\,d\gamma_n \,\|\,d\gamma_n)$.
Then the classical log-Sobolev inequality in Gaussian space asserts that</p>

<p>\begin{equation}\label{eq:logsob}
\mathrm{Ent}_{\gamma_n}(f) \leq \frac12 \int \frac{|\nabla f|^2}{f}\,d\gamma_n\,.
\end{equation}</p>

<p>First, we discuss the correct way to interpret this.
Define the Ornstein-Uhlenbeck semi-group $\{U_t\}$ by its action
\[
U_t f(x) = \mathbb{E}[f(e^{-t} x + \sqrt{1-e^{-2t}} B_1)]\,.
\]
This is the natural stationary diffusion process on Gaussian space.  For every measurable $f$, we have
\[
U_t f \to \int f d\gamma_n \quad \textrm{ as $t \to \infty$}\,,
\]
or equivalently
\[
\mathrm{Ent}_{\gamma_n}(U_t f) \to 0 \quad \textrm{ as $t \to \infty$}\,.
\]</p>

<p>
The log-Sobolev inequality yields quantitative convergence in the relative entropy
distance as follows:
Define the <em>Fisher information</em>
\[
I(f) = \int \frac{\|\nabla f\|^2}{f} \,d\gamma_n\,.
\]
</p>

<p>
One can check that
$$
\frac{d}{dt} \mathrm{Ent}_{\gamma_n} (U_t f)\Big|_{t=0} = - I(f)\,,
$$
thus the Fisher information describes the instantaneous decay of the relative entropy of $f$
under diffusion.
</p>

<p>
So we can rewrite the log-Sobolev inequality as:
\[
- \frac{d}{dt} \mathrm{Ent}_{\gamma_n}(U_t f)\Big|_{t=0} \geq \mathrm{Ent}_{\gamma_n}(f)\,.
\]
This expresses the intuitive fact that when the relative entropy is large,
its rate of decay toward equilibrium is faster.
</p>

                    ]]>
                </content:encoded>
                <guid isPermaLink="false">/entropy/2015/11/21/follmer-drift/</guid>
                <description>
                    
                    F&amp;ouml;llmer's drift, Ito's lemma, and the Gaussian log-Sobolev inequality
                    
                </description>
                <pubDate>Sat, 21 Nov 2015 00:00:00 -0800</pubDate>
                <author>James R. Lee</author>
            </item>
        
    
  </channel>
</rss>
