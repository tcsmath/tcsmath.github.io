<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tcs math</title>
    <description>some mathematics &amp; computation</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>tcsmath relaunch</title>
        <description>&lt;p&gt;I am relaunching tcsmath on a new platform in anticipation
of resuming regular posting.  The old pages
should still be available here &lt;a href=&quot;https://tcsmath.wordpress.com/&quot;&gt;tcsmath&lt;/a&gt;,
and some of them will be slowly migrated to the new format.
There may be some DNS/https hiccups.  I hope those are resolved soon.&lt;/p&gt;

</description>
        <pubDate>Sat, 31 Mar 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/admin/2018/03/31/relaunch/</link>
        <guid isPermaLink="true">http://localhost:4000/admin/2018/03/31/relaunch/</guid>
      </item>
    
      <item>
        <title>Regret minimization and competitive analysis</title>
        <description>&lt;p&gt;These are notes for the first lecture of a course I am co-teaching with &lt;a href=&quot;https://sbubeck.com/&quot;&gt;Seb Bubeck&lt;/a&gt; on
&lt;a href=&quot;https://homes.cs.washington.edu/~jrl/teaching/cse599I-spring-2018/&quot;&gt;Competitive analysis via convex optimization&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I want to set the groundwork by reviewing the bandits model in online learning
and the standard exponential weights algorithm, and then trying to extend it
to the setting of competitive analysis where the analogous
problem goes by the name of &lt;em&gt;metrical task systems&lt;/em&gt;.
Seeing where things go wrong will give us a plan to follow for much of the course.
Some of the objects and algorithms in this lecture may appear
unmotivated; that will be addressed soon.&lt;/p&gt;

&lt;h2 id=&quot;regret-minimization&quot;&gt;Regret minimization&lt;/h2&gt;

&lt;p&gt;Here we review quickly the basic &lt;em&gt;multi-arm bandits model&lt;/em&gt; in online learning.
For more background see, e.g., this &lt;a href=&quot;http://sbubeck.com/SurveyBCB12.pdf&quot;&gt;survey of Bubeck and Cesa-Binachi&lt;/a&gt;.
&lt;script type=&quot;math/tex&quot;&gt;\def\seteq{\mathrel{\vcenter{:}}=}
\def\cE{\mathcal{E}}
\def\R{\mathbb{R}}
\def\argmin{\mathrm{argmin}}
\def\llangle{\left\langle}
\def\rrangle{\right\rangle}
\def\1{\mathbf{1}}
\def\e{\varepsilon}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;In the standard bandits model, we have a set of
experts $\cE = \{1,2,\ldots,N\}$, and
loss functions
$\ell_1,\ell_2,\ldots$ arriving over time,
where $\ell_t : \cE \to [0,1]$.&lt;/p&gt;

&lt;p&gt;For the sake of simplicity, we will work in the full
information model.  At time $t \geq 1$, we have seen $\ell_1,\ldots,\ell_{t-1}$.
We choose some strategy $x_t \in \cE$ and incur cost $\ell_t(x_t)$.
Our goal is to minimize the total cost incurred:
$\sum_{t=1}^T \ell_t(x_t)$.
In the setting of adversarial bandits, we will allow ourselves
to employ a randomized strategy, playing instead a distribution $p_t : \cE \to [0,1]$ at time $t$.&lt;/p&gt;

&lt;p&gt;In the regret minimization framework, we compare our expected loss to
that of the optimal fixed strategy:  The &lt;em&gt;regret incurred up to time $T$&lt;/em&gt; is&lt;/p&gt;
&lt;p&gt;
$$R_T \seteq \sum_{t=1}^T \langle p_t, \ell_t\rangle - \min_{x \in \cE} \sum_{t=1}^T \ell_t(x)\,,$$
&lt;/p&gt;
&lt;p&gt;where for $f,g : \cE \to \R$, we write $\langle f,g\rangle = \sum_{y \in \cE} f(y) g(y)$.&lt;/p&gt;

&lt;p&gt;We will bound the regret in two steps.
Denote $x_T^* \seteq \argmin_{x \in \cE} \sum_{t=1}^T \ell_t(x)$.  Then:&lt;/p&gt;
&lt;p&gt;
\begin{align*}
   R_T &amp;amp;= \sum_{t=1}^T \langle p_t - p_{t+1},\ell_t\rangle + 
   \sum_{t=1}^T \langle p_{t+1}, \ell_t-\ell_t(x_T^*)\rangle \\
   &amp;amp;\leq \sum_{t=1}^T \|p_t-p_{t+1}\|_1 + 
   \sum_{t=1}^T \langle p_{t+1}, \ell_t-\ell_t(x_T^*)\rangle,
\end{align*}
&lt;/p&gt;
&lt;p&gt;where the inequality uses that the losses lie in $[0,1]$.&lt;/p&gt;

&lt;h3 id=&quot;continuous-time-dynamics&quot;&gt;Continuous-time dynamics&lt;/h3&gt;

&lt;p&gt;For a number of reasons, the analysis will be much cleaner in continuous time.
We can think about the losses as a trajectory $\left\{ \ell_t : \cE \to [0,1] \mid t \geq 0 \right\}$,
where the instantaneous loss incurred is $\ell_t\,dt$.
To recover the discrete time setting, simply consider a trajectory $\{ \ell_t : t \geq 0 \}$
that is piecewise-constant.&lt;/p&gt;

&lt;p&gt;Now we can analogously bound the regret:&lt;/p&gt;
&lt;p&gt;
\begin{equation}\label{eq:cont-regret}
   R_T \leq \int_0^T \|\partial_t p_t\|_1 \,dt + \int_0^T \langle p_t, \ell_t-\ell_t(x_T^*)\rangle\,dt\,,
\end{equation}
&lt;/p&gt;
&lt;p&gt;where $x_T^* \seteq \argmin_{x \in \cE} \int_0^T \ell_t(x)\,dt$.&lt;/p&gt;

&lt;p&gt;We will start with $p_t(x) = 1/N$ for every $x \in \cE$, and
employ the following exponential-weights strategy for updating $p_t$:
\begin{equation}\label{eq:dynamics}
\partial_t \log p_t(x) = \eta\left( - \ell_t(x) + \langle p_t, \ell_t\rangle\right),
\end{equation}
where $\eta &amp;gt; 0$ is a parameter called the &lt;em&gt;learning rate&lt;/em&gt; that we will choose soon.
This algorithm will be motivated more in the next lecture, but for now one can note that
we are simply doing continuous-time gradient descent on the vector $\log p_t(x)$:  We are moving
in the direction $- \eta \ell_t dt$.  The additional additive term in \eqref{eq:dynamics}
is there to maintain the constraint that $p_t$ is a probability distribution.&lt;/p&gt;

&lt;p&gt;One can see this clearly by rewriting \eqref{eq:dynamics} as:
\begin{equation}\label{eq:exp-form}
   \partial_t p_t(x) = p_t(x) \, \eta \left(- \ell_t(x) + \langle p_t,\ell_t\rangle\right),
\end{equation}
so that $\sum_{x \in \cE} \partial_t p_t(x) = 0$.&lt;/p&gt;

&lt;h3 id=&quot;the-lyapunov-functional-aka-the-potential&quot;&gt;The Lyapunov functional, aka the potential&lt;/h3&gt;

&lt;p&gt;In order to bound the regret $R_T$, we will employ the philosophy underlying
the entire course:  If we are incurring more cost than $x_T^*$, we would like to be &lt;em&gt;learning&lt;/em&gt;
about $x_T^*$ in a suitable sense.&lt;/p&gt;

&lt;p&gt;Define:
[
   D(x; p) \seteq - \log p(x)\,.
]
Note that for any $x \in \cE$, we have $D(x; p_0) = \log N$, and $D(x; p_t) \geq 0$ for all $t \geq 0$.&lt;/p&gt;

&lt;p&gt;For any $x \in \cE$:
\begin{equation}\label{eq:lya}
   \partial_t D(x;p_t) = - \partial_t \log p_t(x) = \eta \left(\ell_t(x) - \langle p_t,\ell_t\rangle \right).
\end{equation}
If we think of $D(x;p_t)$ as the “distance” from $p_t$ to $x$, then our distance to $x$
is decreasing proportional to the advantage $x$ has over our strategy $p_t$.
Integrating \eqref{eq:lya} over $[0,T]$ gives
[
   D(x; p_0) - D(x; p_T) = \eta \int_0^T \langle p_t, \ell_t-\ell_t(x)\rangle\,dt
]&lt;/p&gt;

&lt;p&gt;Applying this with $x=x_T^*$ and recalling \eqref{eq:cont-regret}, we have&lt;/p&gt;
&lt;p&gt;
\begin{align*}
   R_T  &amp;amp;\leq \int_0^T \|\partial_t p_t\|_1\,dt + \frac{D(x_0^*;p_0)-D(x_T^*;p_T)}{\eta} \\
        &amp;amp;\leq \int_0^T \|\partial_t p_t\|_1\,dt + \frac{\log N}{\eta} \\
        &amp;amp;\leq \eta T + \frac{\log N}{\eta}\,,
\end{align*}
&lt;/p&gt;
&lt;p&gt;where the last inequality uses \eqref{eq:exp-form} and the fact that the losses are in $[0,1]$
to write $\|\partial_t p_t\|_1 \leq \eta \|p_t\|_1 = \eta$.&lt;/p&gt;

&lt;p&gt;Setting $\eta = \sqrt{\frac{\log N}{T}}$ yields the standard regret bound
[
   R_T \leq 2 \sqrt{T \log N}\,.
]
If one does not know the final time $T$, it is not difficult to see that one can choose
a time-dependent learning rate $\eta=\eta(t)=\sqrt{\frac{\log N}{t}}$ to obtain a similar result.
result&lt;/p&gt;

&lt;h2 id=&quot;competitive-analysis&quot;&gt;Competitive analysis&lt;/h2&gt;

&lt;p&gt;The competitive analysis analog of the bandits framework goes by the name of
&lt;em&gt;metrical task systems (MTS)&lt;/em&gt;.  This problem was introduced in 1992 by
&lt;a href=&quot;http://www.cs.huji.ac.il/~nati/PAPERS/bls_online.pdf&quot;&gt;Borodin, Linial, and Saks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This setting has three major differences:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;At each time $t \geq 1$, we receive a &lt;em&gt;cost function&lt;/em&gt; $c_t : \cE \to [0,\infty)$, and we choose
an action $x_t \in \cE$ &lt;em&gt;after&lt;/em&gt; receiving $c_t$.&lt;/li&gt;
  &lt;li&gt;There is a metric $d$ on $\cE$ that makes $(\cE,d)$ into a metric space,
and in addition to the &lt;em&gt;service cost&lt;/em&gt; $c_t(x_t)$, we pay a &lt;em&gt;switching cost&lt;/em&gt; $d(x_{t-1},x_t)$
for playing $x_t$.&lt;/li&gt;
  &lt;li&gt;We compare ourselves against an offline optimum that has the &lt;em&gt;same ability to switch strategies&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Say that an online algorithm $\llangle x_1, x_2, \ldots,x_T\rrangle$ is $\alpha$-competitive
if, for every $x_0 \in \cE$ and every cost sequence $c_1,c_2,\ldots,c_T$, it holds that&lt;/p&gt;
&lt;p&gt;
$$
   \sum_{t=1}^T c_t(x_t) + d(x_{t-1},x_t) \leq \alpha\left(\sum_{t=1}^T c_t(x^*_t) + d(x_{t-1}^*, x_t^*)\right) + O(1)\,,
$$
&lt;/p&gt;
&lt;p&gt;where $\llangle x_0^*, x_1^*, x_2^*, \ldots, x_T^*\rrangle$ is the optimal &lt;em&gt;offline&lt;/em&gt; strategy
with $x_0^*=x_0$, i.e., the optimal strategy in hindsight.  The additive $O(1)$ term is a constant
that should be independent of the request sequence.&lt;/p&gt;

&lt;p&gt;It is conjectured that the competitive ratio is $O(\log N)$ for every $N$-point metric space.
In the coming weeks, we will see an $O((\log N)^2)$-competitive algorithm
based on upcoming joint work with Bubeck, Cohen, and Y. T. Lee.
This improves slightly on the $O((\log N)^2 \log \log N)$ bound of &lt;a href=&quot;https://arxiv.org/abs/cs/0406034&quot;&gt;Fiat and Mendel&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;attempting-exponential-weights&quot;&gt;Attempting exponential weights&lt;/h3&gt;

&lt;p&gt;The simplest setting for MTS is when the metric $(\cE,d)$ is uniform, i.e., $d(x,y)=\1_{\{x\neq y\}}$ for $x,y \in \cE$.&lt;/p&gt;

&lt;p&gt;Just as in the bandits setting, we can consider a continuous-time trajectory
$\left\{ c_t : \cE \to [0,\infty) \mid t \geq 0\right\}$ on cost
functions.  And we might try to employ a similar strategy:
\begin{equation}\label{eq:cost-dynamics}
   \partial_t \log p_t(x) = - c_t(x) + \langle p_t, c_t\rangle.
\end{equation}&lt;/p&gt;

&lt;p&gt;But now we run into a major hurdle:  Such an algorithm cannot be $\alpha$-competitive for any $\alpha &amp;lt; \infty$.&lt;/p&gt;

&lt;p&gt;
Suppose we arrange that for some $x_0 \in \cE$ and $t_0 &amp;gt; 0$ and small $\epsilon &amp;gt; 0$,
it holds that $p_{t_0}(x_0) = 1-\epsilon$.
We can do this by having an adversary play the cost function $c(x)=\1_{\cE \setminus \{x_0\}}(x)$
for a long enough period of time
so that any competitive algorithm must put almost all the probability mass on $x_0$.
&lt;/p&gt;

&lt;p&gt;
Now the adversary changes the cost function to $c(x)=\1_{x_0}(x)$.
The dynamics specified by \eqref{eq:cost-dynamics} will move much too slowly!
Indeed:
\[
   \partial_t \log p_t(x_0) = -1+(1-p_t(x_0)) = - p_t(x_0),
\]
i.e.,
\[
   \partial_t p_t(x_0) = - p_t(x_0)^2,
\]
thus it will take roughly $1/\epsilon$ time before $p_t(x_0) &amp;lt; 1/2$.
&lt;/p&gt;

&lt;p&gt;Thus our algorithm will incur cost $\asymp 1/\epsilon$ while the optimal offline algorithm incurs cost $O(1)$.
In the bandits setting, this was fine because every fixed strategy incurs cost $\asymp 1/\epsilon$.
But in the setting of competitive analysis, our algorithm needs to be a lot more nimble
to keep up with an offline algorithm that can switch strategies.&lt;/p&gt;

&lt;h3 id=&quot;the-exploration-shift&quot;&gt;The exploration shift&lt;/h3&gt;

&lt;p&gt;To fix this, we will design an algorithm that devotes a constant fraction of the service cost
it is currently incurring to exploring the strategy space.
Essentially, this can be achieved by pretending that $p_t(x) \geq 1/(2N)$ for every $x \in \cE$.
(Recall that $N = |\cE|$.)
This transformation (mixing with the uniform distribution)
is not uncommon in the bandit literature.
In the setting of metrical task systems, I saw it for the first time
in this paper of &lt;a href=&quot;/assets/papers/pot.pdf&quot;&gt;Bansal, Buchbinder, and Naor&lt;/a&gt; on the weighted paging problem.&lt;/p&gt;

&lt;p&gt;Define $p_0(x)=\1_{x_0}(x)$ where $x_0 \in X$ is the starting point.
Let $\delta &amp;gt; 0$ be a number we will choose soon,
and consider the dynamics:
\begin{equation}\label{eq:mts-dynamics}
   \partial_t \log (p_t(x)+\delta) = - \hat{c}_t(x) + \llangle \frac{p_t+\delta}{1+\delta N}, \hat{c}_t\rrangle\,,
\end{equation}
which can be written equivalently as
\begin{equation}\label{eq:mts-move}
   \partial_t p_t(x) = (p_t(x)+\delta) \left(- \hat{c}_t(x) + 
   \llangle \frac{p_t+\delta}{1+\delta N}, \hat{c}_t\rrangle\right).
\end{equation}
A natural choice is to take $\hat{c}_t(x)=c_t(x)$, but this presents a problem:
Unlike \eqref{eq:cost-dynamics}, these dynamics no longer enforce
that $p_t(x) \geq 0$.&lt;/p&gt;

&lt;h4 id=&quot;lagrangian-multipliers&quot;&gt;Lagrangian multipliers&lt;/h4&gt;

&lt;p&gt;In this relatively simple setting,
we can consider reduced costs
$\hat{c}_t(x) \seteq c_t(x)-\lambda_t(x)$ satisfying:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\lambda_t(x) \geq 0$ for all $t \geq 0$,&lt;/li&gt;
  &lt;li&gt;$p_t(x) = 0 \implies \partial_t p_t(x) \geq 0$,&lt;/li&gt;
  &lt;li&gt;$\lambda_t(x) &amp;gt; 0 \implies p_t(x)=0$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With these constraints in place,
the corresponding trajectory $\{p_t : t \geq 0\}$
will always be a probability measure,
and we can charge ourselves $\hat{c}_t(x)$ without
worrying about cheating, since $p_t(x) \hat{c}_t(x) = p_t(x) c_t(x)$
will always hold.
The existence of functions $\{ \lambda_t : \cE \to [0,\infty) \mid t \geq 0\}$
is a slightly subtle issue that will be addressed formally
in the coming lectures.  These are Lagrangian multipliers corresponding
to the constraints $p_t(x) \geq 0$ for $x \in \cE$.&lt;/p&gt;

&lt;p&gt;As we will see in future lectures, the Lagrangian multipliers
will not adversely affect the potential analysis,
but they could cause our algorithm to incur movement cost.
In the present setting, things are going in the beneficial direction:
The multipliers correspond to &lt;em&gt;reduced&lt;/em&gt; costs, and therefore
they actually slow down our movement.&lt;/p&gt;

&lt;h4 id=&quot;the-potential-analysis&quot;&gt;The potential analysis&lt;/h4&gt;

&lt;p&gt;
Define now the potential
\[
   D_{\delta}(x; p) \seteq - \log (p(x)+\delta).
\]
We are interested in $\partial_t D_{\delta}(x_t^*; p_t)$.
Let's first consider the derivative with respect to $x_t^*$.
For any $x,y \in \cE$:
\[
   \left|D_{\delta}(x; p) - D_{\delta}(y ;p)\right| \leq \log(1/\delta)\,.
\]
Thus for any $T \geq 0$:
\begin{equation}\label{eq:first-mts}
   D_{\delta}(x_{T}^*; p_T) - D_{\delta}(x_0^*, p_0)
   \leq \log(1/\delta) \sum_{t=1}^{\lfloor T\rfloor} d(x_t^*, x_{t-1}^*) + \int_0^T \partial_t D_{\delta}\left(x^*_{\lfloor t\rfloor}; p_t\right)\,dt\,.
\end{equation}
To analyze the latter term, use \eqref{eq:mts-dynamics}
to observe that for any $x \in \cE$,
\[
   \partial_t D_{\delta}(x;p_t) = \hat{c}_t(x) - \llangle \frac{p_t+\delta}{1+\delta N}, \hat{c}_t\rrangle,
\]
hence
\[
   \int_0^T \partial_t D_{\delta}\left(x_{\lfloor t\rfloor}^*; p_t\right)\,dt =
   \int_0^T \hat{c}_t(x_t^*)\,dt - \int_0^T \llangle \frac{p_t+\delta}{1+\delta N}, \hat{c}_t\rrangle\,dt\,.
\]
Plugging this into \eqref{eq:first-mts} and rearranging gives
\begin{align}\nonumber
   (1+&amp;amp; \delta N)^{-1} \int_0^T \langle p_t + \delta, \hat{c}_t\rangle\,dt \\
&amp;amp;\leq 
   \left[ D_{\delta}(x_{0}^*; p_0) - D_{\delta}(x_T^*, p_T) \right]
    + \log(1/\delta) \sum_{t=1}^{\lfloor T\rfloor} d(x_t^*, x_{t-1}^*) + 
   \int_0^T \hat{c}_t(x_t^*)\,dt  \nonumber \\
   &amp;amp;\leq
    \log(1/\delta) \sum_{t=1}^{\lfloor T\rfloor} d(x_t^*, x_{t-1}^*) + 
    \int_0^T c_t(x_t^*)\,dt\,,\label{eq:second-mts}
\end{align}
where we have used the fact that the term in brackets is nonpositive,
and $c_t \geq \hat{c}_t$ pointwise.
This looks great:  We have
bounded the service cost of our algorithm by the movement and service
costs of the optimal algorithm.
We are left to consider the movement cost of the algorithm.
&lt;/p&gt;

&lt;h3 id=&quot;the-movement-cost&quot;&gt;The movement cost&lt;/h3&gt;

&lt;p&gt;
Here we use a trick from online algorithms:
Instead of bounding the total movement cost
$\int_0^T \|\partial_t p_t\|_1\,dt,$
we will bound only the &lt;em&gt;incoming movement&lt;/em&gt;
$\int_0^T \|\left(\partial_t p_t\right)_+\|_1\,dt$.
&lt;/p&gt;

&lt;p&gt;Since “what goes in must come out (unless it stays there forever),” we have:&lt;/p&gt;
&lt;p&gt;
$$
   \int_0^T \|\partial_t p_t\|_1\,dt \leq
   2 \int_0^T \|\left(\partial_t p_t\right)_+\|_1\,dt + 1\,.
$$
And now \eqref{eq:mts-move} gives
$$
   \left\|\left(\partial_t p_t\right)_+\right\|_1 \leq \langle p_t+\delta,\hat{c}_t\rangle\,.
$$
&lt;/p&gt;

&lt;p&gt;
Combining this with \eqref{eq:second-mts} and using the fact that $\langle p_t, c_t\rangle = \langle p_t, \hat{c}_t\rangle$
yields
\begin{align*}
   \int_0^T \left(\langle p_t,c_t\rangle + \|\partial_t p_t\|_1\right)\,dt &amp;amp;\leq
   1 + 3 \int_0^T \langle p_t + \delta ,\hat{c}_t\rangle\,dt \\
   &amp;amp; \leq 1 + 3(1+\delta N) \left[\log(1/\delta) \sum_{t=1}^{\lfloor T\rfloor} d(x_t^*, x_{t-1}^*) + 
\int_0^T c_t(x_t^*)\,dt\right].
\end{align*}
Thus setting $\delta = 1/N$ yields an $O(\log N)$-competitive algorithm for MTS on a uniform metric space.
&lt;/p&gt;

&lt;h4 id=&quot;a-comment-about-absolute-continuity&quot;&gt;A comment about absolute continuity&lt;/h4&gt;

&lt;p&gt;Note that in \eqref{eq:first-mts}, we have used the fundamental theorem of calculus
to integrate a derivative.
In order for this to be valid, it must be that $\log p_t(x)$ is absolutely continuous
as a function of $t$.
When we argue formally about the existence of the Lagrangian multipliers $\lambda_t$,
we will need to ensure that the resulting trajectory is absolutely continuous.&lt;/p&gt;

</description>
        <pubDate>Sat, 31 Mar 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000/online/2018/03/31/competitive-analysis/</link>
        <guid isPermaLink="true">http://localhost:4000/online/2018/03/31/competitive-analysis/</guid>
      </item>
    
      <item>
        <title>An entropy optimal drift</title>
        <description>&lt;h2 id=&quot;construction-of-föllmers-drift&quot;&gt;Construction of Föllmer’s drift&lt;/h2&gt;

&lt;p&gt;In a previous post, we saw how an entropy-optimal drift process
could be used to prove the Brascamp-Lieb inequalities.
Our main tool was a result of Föllmer that we now recall and justify.
Afterward, we will use it to prove the Gaussian log-Sobolev inequality.&lt;/p&gt;

&lt;p&gt;Consider $f : \mathbb R^n \to \mathbb R_+$ with $\int f \,d\gamma_n = 1$,
where $\gamma_n$ is the standard Gaussian measure on $\mathbb R^n$.
Let $\{B_t\}$ denote an $n$-dimensional Brownian motion with $B_0=0$.
We consider all processes of the form
\begin{equation}\label{eq:drift}
W_t = B_t + \int_0^t v_s\,ds\,,
\end{equation}
where $\{v_s\}$ is a progressively measurable drift
and such that $W_1$ has law $f\,d\gamma_n$.&lt;/p&gt;

&lt;div class=&quot;theorem&quot; text=&quot;Energy-entropy&quot;&gt;
It holds that
\[
D(f d\gamma_n \,\|\, d\gamma_n) = \min D(W_{[0,1]} \,\|\, B_{[0,1]}) = \min \frac12 \int_0^1 \mathbb{E}\,\|v_t\|^2\,dt\,,
\]
where the minima are over all processes of the form \eqref{eq:drift}.
&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;
In the preceding post (Lemma 2), we have already seen that
for any drift of the form \eqref{eq:drift}, it holds that
\[
D(f d\gamma_n \,\|\,d\gamma_n) \leq \frac12 \int_0^1 \mathbb{E}\,\|v_t\|^2\,dt \leq D(W_{[0,1]} \,\|\, B_{[0,1]})\,,
\]
thus we need only exhibit a drift $\\{v_t\\}$ achieving equality.

&lt;p&gt;
We define
\[
v_t = \nabla \log P_{1-t} f(W_t) = \frac{\nabla P_{1-t} f(W_t)}{P_{1-t} f(W_t)}\,,
\]
where $\\{P_t\\}$ is the Brownian semigroup defined by
\[
P_t f(x) = \mathbb{E}[f(x + B_t)]\,.
\]
&lt;/p&gt;

&lt;p&gt;
Note that $v_t$ is almost surely constant conditioned on the past, hence the chain rule yields
\begin{equation}\label{eq:chain}
D(W_{[0,1]} \,\|\, B_{[0,1]}) =
\frac12 \int_0^1 \mathbb{E}\,\|v_t\|^2\,dt\,.
\end{equation}
(See line (7) of Lemma 2 in the previous post.  Note that $h(v_t)=0$ since $v_t$ is deterministic given the past.)
We are left to show that $W_1$ has law $f \,d\gamma_n$ and $D(W_{[0,1]} \,\|\, B_{[0,1]}) = D(f d\gamma_n \,\|\,d\gamma_n)$.
&lt;/p&gt;

&lt;p&gt;
We will prove the first fact using Girsanov's theorem to argue about
the change of measure between $\{W_t\}$ and $\{B_t\}$.
As in the previous post, we will argue somewhat informally
using the heuristic that the law of $dB_t$ is a Gaussian
random variable in $\mathbb R^n$ with covariance $dt \cdot I$.
It&amp;ocirc;'s formula states that this heuristic is justified (see our use
of the formula below).
&lt;/p&gt;

The following lemma says that, given any sample path $\{W_s : s \in [0,t]\}$
of our process up to time $s$, the probability that Brownian motion (without drift)
would have
&quot;done the same thing is $\frac{1}{M_t}$.
&lt;/div&gt;

&lt;div class=&quot;remark&quot;&gt;
I chose to present various steps in the next proof at varying levels of formality.
The arguments have the same structure as corresponding formal proofs,
but I thought (perhaps na&amp;iuml;vely) that this would be instructive.
&lt;/div&gt;

&lt;div class=&quot;lemma&quot; text=&quot;Heuristic Girsanov&quot;&gt;
Let $\mu_t$ denote the law of $\\{W_s : s \in [0,t]\\}$.
If we define
\[
M_t = \exp\left(-\int_0^t \langle v_s,dB_s\rangle - \frac12 \int_0^t \|v_s\|^2\,ds\right)\,,
\]
then under the measure $\nu_t$ given by
\[
d\nu_t = M_t \,d\mu_t\,,
\]
the process $\\{W_s : s \in [0,t]\\}$ has the same law as $\\{B_s : s \in [0,t]\\}$.
&lt;/div&gt;

&lt;div class=&quot;proof&quot;&gt;
We argue by analogy with the discrete proof.
First, let us define the infinitesimal ``transition kernel'' of Brownian motion
using our heuristic that $dB_t$ has covariance $dt \cdot I$:
\[
p(x,y) = \frac{e^{-\|x-y\|^2/2dt}}{(2\pi dt)^{n/2}}\,.
\]
&lt;p&gt;
We can also compute the (time-inhomogeneous) transition kernel $q_t$ of $\\{W_t\\}$:
\[
q_t(x,y) =  \frac{e^{-\|v_t dt + x - y\|^2/2dt}}{(2\pi dt)^{n/2}} = p(x,y) e^{-\frac12 \|v_t\|^2 dt} e^{-\langle v_t, x-y\rangle}\,.
\]
Here we are using that $dW_t = dB_t + v_t\,dt$ and $v_t$ is deterministic conditioned on the past, thus
the law of $dW_t$ is a normal with mean $v_t\,dt$ and covariance $dt \cdot I$.
&lt;/p&gt;

&lt;p&gt;
To avoid confusion of derivatives, let's use $\alpha_t$ for the density of $\mu_t$ and $\beta_t$ for the density of
Brownian motion (recall that these are densities on paths).
Now let us relate the density $\alpha_{t+dt}$ to the density $\alpha_{t}$.
We use here the notations $\\{\hat W_t, \hat v_t, \hat B_t\\}$ to denote
a (non-random) sample path of $\\{W_t\\}$:
\begin{align*}
\alpha_{t+dt}(\hat W_{[0,t+dt]}) &amp;amp;= \alpha_t(\hat W_{[0,t]})  q_t(\hat W_t, \hat W_{t+dt}) \\
&amp;amp;=  \alpha_t(\hat W_{[0,t]}) p(\hat W_t, \hat W_{t+dt}) e^{-\frac12 \|\hat v_t\|^2\,dt-\langle \hat v_t,\hat W_t-\hat W_{t+dt}\rangle} \\
&amp;amp;=
\alpha_t(\hat W_{[0,t]})  p(\hat W_t, \hat W_{t+dt}) e^{-\frac12 \|\hat v_t\|^2\,dt+\langle \hat v_t,d \hat W_t\rangle} \\
&amp;amp;=
\alpha_t(\hat W_{[0,t]})  p(\hat W_t, \hat W_{t+dt}) e^{\frac12 \|\hat v_t\|^2\,dt+\langle \hat v_t, d \hat B_t\rangle}\,,
\end{align*}
where the last line uses $d\hat W_t = d\hat B_t + \hat v_t\,dt$.
&lt;/p&gt;

Now by ``heuristic'' induction, we can assume $\alpha_t(\hat W_{[0,t]})=\frac{1}{M_t} \beta_t(\hat W_{[0,t]})$, yielding
\begin{align*}
\alpha_{t+dt}(\hat W_{[0,t+dt]}) &amp;amp;= \frac{1}{M_t} \beta_t(\hat W_{[0,t]})  p(\hat W_t, \hat W_{t+dt}) e^{\frac12 \|\hat v_t\|^2\,dt+\langle \hat v_t, d \hat B_t\rangle} \\
&amp;amp;=
\frac{1}{M_{t+dt}}  \beta_t(\hat W_{[0,t]}) p(\hat W_t, \hat W_{t+dt}) \\
&amp;amp;=
\frac{1}{M_{t+dt}}  \beta_{t+dt}(\hat W_{[0,t+dt]})\,.
\end{align*}
In the last line, we used the fact that $p$ is the infinitesimal transition kernel for Brownian motion.
&lt;/div&gt;

&lt;h2 id=&quot;the-gaussian-log-sobolev-inequality&quot;&gt;The Gaussian log-Sobolev inequality&lt;/h2&gt;

&lt;p&gt;Consider again a measurable $f : \mathbb R^n \to \mathbb R_+$ with $\int f\,d\gamma_n=1$.
Let us define $\mathrm{Ent}_{\gamma_n}(f) = D(f\,d\gamma_n \,\|\,d\gamma_n)$.
Then the classical log-Sobolev inequality in Gaussian space asserts that&lt;/p&gt;

&lt;p&gt;\begin{equation}\label{eq:logsob}
\mathrm{Ent}_{\gamma_n}(f) \leq \frac12 \int \frac{|\nabla f|^2}{f}\,d\gamma_n\,.
\end{equation}&lt;/p&gt;

&lt;p&gt;First, we discuss the correct way to interpret this.
Define the Ornstein-Uhlenbeck semi-group $\{U_t\}$ by its action
\[
U_t f(x) = \mathbb{E}[f(e^{-t} x + \sqrt{1-e^{-2t}} B_1)]\,.
\]
This is the natural stationary diffusion process on Gaussian space.  For every measurable $f$, we have
\[
U_t f \to \int f d\gamma_n \quad \textrm{ as $t \to \infty$}\,,
\]
or equivalently
\[
\mathrm{Ent}_{\gamma_n}(U_t f) \to 0 \quad \textrm{ as $t \to \infty$}\,.
\]&lt;/p&gt;

&lt;p&gt;
The log-Sobolev inequality yields quantitative convergence in the relative entropy
distance as follows:
Define the &lt;em&gt;Fisher information&lt;/em&gt;
\[
I(f) = \int \frac{\|\nabla f\|^2}{f} \,d\gamma_n\,.
\]
&lt;/p&gt;

&lt;p&gt;
One can check that
$$
\frac{d}{dt} \mathrm{Ent}_{\gamma_n} (U_t f)\Big|_{t=0} = - I(f)\,,
$$
thus the Fisher information describes the instantaneous decay of the relative entropy of $f$
under diffusion.
&lt;/p&gt;

&lt;p&gt;
So we can rewrite the log-Sobolev inequality as:
\[
- \frac{d}{dt} \mathrm{Ent}_{\gamma_n}(U_t f)\Big|_{t=0} \geq \mathrm{Ent}_{\gamma_n}(f)\,.
\]
This expresses the intuitive fact that when the relative entropy is large,
its rate of decay toward equilibrium is faster.
&lt;/p&gt;
</description>
        <pubDate>Sat, 21 Nov 2015 00:00:00 -0800</pubDate>
        <link>http://localhost:4000/entropy/2015/11/21/follmer-drift/</link>
        <guid isPermaLink="true">http://localhost:4000/entropy/2015/11/21/follmer-drift/</guid>
      </item>
    
  </channel>
</rss>
