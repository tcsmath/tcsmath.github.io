<!DOCTYPE html>
<html>
<head>
   <!-- Global site tag (gtag.js) - Google Analytics -->
   <script async src="https://www.googletagmanager.com/gtag/js?id=UA-57020016-1"></script>
   <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-57020016-1');
   </script>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Approximation by ultrametrics &#8211; tcs math</title>
    <link rel="dns-prefetch" href="//maxcdn.bootstrapcdn.com">
    <link rel="dns-prefetch" href="//cdnjs.cloudflare.com">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="The competitive ratio for MTS is controlled by ultrametrics">
    <meta name="robots" content="all">
    <meta name="author" content="James R. Lee">
    
    <meta name="keywords" content="online">
    <link rel="canonical" href="http://localhost:4000/online/2018/04/12/metric-approx/">
    <link rel="alternate" type="application/rss+xml" title="RSS Feed for tcs math" href="/feed.xml" />

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/pixyll.css?201804122040" type="text/css">

    <!-- Fonts -->
    
    <link href='//fonts.googleapis.com/css?family=Merriweather:900,900italic,300,300italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Lato:900,300' rel='stylesheet' type='text/css'>
    
    
      <link href="//maxcdn.bootstrapcdn.com/font-awesome/latest/css/font-awesome.min.css" rel="stylesheet">
    

    <!-- MathJax -->
    
    <script type="text/javascript"
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML">
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } } });
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\[","\]"], ["\\[","\\]"] ],
          processEscapes: true
        },
        messageStyle: "none",
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>

    

    <!-- Verifications -->
    
    

    <!-- Open Graph -->
    <!-- From: https://github.com/mmistakes/hpstr-jekyll-theme/blob/master/_includes/head.html -->
    <meta property="og:locale" content="en_US">
    <meta property="og:type" content="article">
    <meta property="og:title" content="Approximation by ultrametrics">
    <meta property="og:description" content="some mathematics &amp; computation">
    <meta property="og:url" content="http://localhost:4000/online/2018/04/12/metric-approx/">
    <meta property="og:site_name" content="tcs math">
    

    <!-- Twitter Card -->
    <meta name="twitter:card" content="summary" />
    
    <meta name="twitter:title" content="Approximation by ultrametrics" />
    <meta name="twitter:description" content="The competitive ratio for MTS is controlled by ultrametrics" />
    <meta name="twitter:url" content="http://localhost:4000/online/2018/04/12/metric-approx/" />
    

    <!-- Icons -->
    <link rel="apple-touch-icon" sizes="57x57" href="/apple-touch-icon-57x57.png">
    <link rel="apple-touch-icon" sizes="114x114" href="/apple-touch-icon-114x114.png">
    <link rel="apple-touch-icon" sizes="72x72" href="/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="144x144" href="/apple-touch-icon-144x144.png">
    <link rel="apple-touch-icon" sizes="60x60" href="/apple-touch-icon-60x60.png">
    <link rel="apple-touch-icon" sizes="120x120" href="/apple-touch-icon-120x120.png">
    <link rel="apple-touch-icon" sizes="76x76" href="/apple-touch-icon-76x76.png">
    <link rel="apple-touch-icon" sizes="152x152" href="/apple-touch-icon-152x152.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon-180x180.png">
    <link rel="icon" type="image/png" href="/favicon-192x192.png" sizes="192x192">
    <link rel="icon" type="image/png" href="/favicon-160x160.png" sizes="160x160">
    <link rel="icon" type="image/png" href="/favicon-96x96.png" sizes="96x96">
    <link rel="icon" type="image/png" href="/favicon-16x16.png" sizes="16x16">
    <link rel="icon" type="image/png" href="/favicon-32x32.png" sizes="32x32">

    
</head>

<body class="site">
  
	

  <div class="site-wrap">
    <header class="site-header px2 px-responsive">
  <div class="mt2 wrap">
    <div class="measure">
       <a href="http://localhost:4000" class="site-title">tcs math</a>
      <nav class="site-nav">
        




    
    
    
    
        <a href="/about/">about</a>
    


    
    
    
    
        <a href="/contact/">contact</a>
    



<a href="https://tcsmath.wordpress.com/">old tcsmath</a>
    

      </nav>
      <div class="clearfix"></div>
      
    </div>
    <div class="measure">
       [by <a href="http://www.cs.washington.edu/homes/jrl/">James R. Lee</a>]
    </div>
  </div>
</header>


    <div class="post p2 p-responsive wrap" role="main">
      <div class="measure">
        <div class="post-header mb2">
  <h1>Approximation by ultrametrics</h1>
  <span class="post-meta">Apr 12, 2018</span>
   
   
      &nbsp;   <span class="share-links">
    
      <a class="fa fa-facebook" href="https://facebook.com/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fonline%2F2018%2F04%2F12%2Fmetric-approx%2F" rel="nofollow" target="_blank" title="Share on Facebook"></a>
    

    
      <a class="fa fa-twitter" href="https://twitter.com/intent/tweet?text=Approximation+by+ultrametrics&amp;url=http%3A%2F%2Flocalhost%3A4000%2Fonline%2F2018%2F04%2F12%2Fmetric-approx%2F" rel="nofollow" target="_blank" title="Share on Twitter"></a>
    

    

    

    

    

    

    

    
  </span>

   
</div>

<article class="post-content mb2">
  <p>We now take a short detour away from mirror descent, and
instead examine how a special class of metric spaces called <em>ultrametrics</em>
control the competitive ratio of MTS in general metric spaces.
Hold onto your seats; we’re about to compress two decades and 
10 papers into a few paragraphs.
For the sake of continuity, bibliographic remarks are held to the end of the post.
<script type="math/tex">\def\e{\varepsilon}</script></p>

<h2 id="metric-approximations">Metric approximations</h2>

<p>For a metric space $(X,d)$, let $\alpha_{\mathrm{mts}}(X,d)$ denote
the best competitive ratio for MTS on $(X,d)$.
Suppose $D$ is some other metric on $X$ that satisfies</p>
<p>
\begin{equation}\label{eq:distortion}
   \frac{D(x,y)}{K} \leq d(x,y) \leq D(x,y) \qquad \forall x,y \in X\,.
\end{equation}
</p>
<p>Then it is straightforward to verify that $\alpha_{\mathrm{mts}}(X,d) \leq K \cdot \alpha_{\mathrm{mts}}(X,D)$.</p>

<p>But there is a weaker form of approximation that still allows for such a conclusion.
Suppose that $\mathbf{D}$ is a <em>random</em> metric that satisfies, for every $x,y \in X$:</p>

<ol>
  <li>With probability one, $\mathbf{D}(x,y) \geq d(x,y)$,</li>
  <li>$\mathbb{E}\left[\mathbf{D}(x,y)\right] \leq K \cdot d(x,y)$.</li>
</ol>

<p>If $\alpha_{\mathrm{mts}}(X,\mathbf{D}) \leq \alpha$ with probability one, we claim that
$\alpha_{\mathrm{mts}}(X,d) \leq K \alpha$.</p>

<p>The algorithm achieving this is as follows:  Sample the random metric $\mathbf{D}$.
Now given a cost sequence $\left\langle c_t : X \to \mathbb{R}_+ \mid t \geq 1\right\rangle$,
let $\left\langle x_0, x_1, x_2, \ldots \right\rangle$ be the random sequence of points produced
by an $\alpha$-competitive randomized algorithm for $(X,\mathbf{D})$.  Then:</p>
<p>
\[
   \mathbb{E}\left[\sum_{t=1}^T \left.\vphantom{\bigoplus}
         \mathbf{D}(x_t, x_{t-1}) \ \right| \mathbf{D}\right] \leq
         \alpha \sum_{t=1}^T \mathbf{D}\!\left(x_t^{*}, x_{t-1}^{*}\right)
         + O(1)\,,
\]
</p>
<p>where $\left\langle x_t^* : t \geq 0\right\rangle$
is an optimal offline sequence for $(X,d)$.
(This may not be the optimal sequence for $(X,\mathbf{D})$, but the algorithm
is certainly competitive against non-optimal sequences as well.)</p>

<p>Note that the expectation here is taken only with respect to the randomness in the online algorithm.
If we take expectation with respect to $\mathbf{D}$ as well, it follows that</p>
<p>
\[
   \mathbb{E}\left[\sum_{t=1}^T d(x_t, x_{t-1})\right] \leq
   \mathbb{E}\left[\sum_{t=1}^T \mathbf{D}\left(x_t^{*}, x_{t-1}^{*}\right)\right]
         + O(1)
         \leq K 
   \sum_{t=1}^T d\!\left(x_t^{*}, x_{t-1}^{*}\right) + O(1)\,,
\]
</p>
<p>where $\left\langle x_t^* : t \geq 0\right\rangle$ is an optimal offline sequence in $(X,d)$.
We have used property (1) of $\mathbf{D}$ for the LHS and property (2) to bound the RHS.</p>

<h2 id="ultrametrics">Ultrametrics</h2>

<p>Let $T=(V,E)$ be a finite, rooted tree, and let $\mathcal{L}$ denote the leaves of $T$.
Suppose $w : V\setminus \mathcal{L} \to \mathbb{R}_+$ is a function that assigns
positive weights to the internal vertices of $T$ such that the vertex
weights are non-increasing along root-leaf paths.
Then one can define a distance on $\mathcal{L}$ by
[
   d_w\left(\ell,\ell’\right) \mathrel{\vcenter{:}}= w\left(\mathrm{lca}(\ell,\ell’)\right).
]
This is an <em><a href="https://en.wikipedia.org/wiki/Ultrametric_space">ultrametric</a></em> on $\mathcal{L}$ (and all finite ultrametrics
arise in this way).</p>

<p>It turns out that ultrametrics essentially control the competitive ratio
for metrical task systems on finite metric spaces.
This follows from the following two facts that
hold for an arbitrary $n$-point metric space $(X,d)$.</p>

<p><a name="thm1"></a></p>
<p class="theorem" text="Random embedding" ord="1">
There is a random ultrametric $\mathbf{D}$ on $(X,d)$ such that (1) and (2) are satisfied with $K \leq O(\log n)$.
</p>

<p>By our earlier remarks, this implies that the competitive ratio for MTS on $(X,d)$
is at most $O(\log n)$ times the competitive ratio for $n$-point ultrametrics.</p>

<p><a name="thm2"></a></p>
<p class="theorem" text="Metric Ramsey" ord="2">
There is a subset $X' \subseteq X$ with $|X'| \geq \sqrt{n}$ and
an ultrametric $D$ on $X'$ such that 
\eqref{eq:distortion} is satisfied with $K \leq O(1)$.
</p>

<p>Since $\alpha_{\mathrm{mts}}(X,d) \geq \alpha_{\mathrm{mts}}(X’,d) \geq \Omega(\alpha_{\mathrm{mts}}(X’,D))$,
lower bounds on the competitive ratio for ultrametrics yield lower bounds for $(X,d)$ as well.
Finally, we remark that MTS on ultrametrics is now well-understood.</p>

<p><a name="thm3"></a></p>
<p class="theorem" text="MTS on ultrametrics" ord="3">
   If $(X,D)$ is an $n$-point ultrametric, then
   \[
      \Omega\left(\frac{\log n}{\log \log n}\right) \leq \alpha_{\mathrm{mts}}(X,D) \leq O(\log n)\,.
   \]
</p>

<p>There are ultrametrics (e.g., as we have seen already, when $(X,D)$ is the uniform metric)
for which the $O(\log n)$ upper bound in <a href="#thm3">Theorem 3</a> is tight.
Whether the LHS can be made $\Omega(\log n)$ is an intriguing open problem;
we will address it in the next lecture.
In conjunction with <a href="#thm1">Theorem 1</a> and <a href="#thm2">Theorem 2</a>,
this yields.</p>

<p><a name="cor4"></a></p>
<p class="corollary" text="MTS" ord="4">
For any $n$-point metric space $(X,d)$:
\[
   \Omega\left(\frac{\log n}{\log \log n}\right) \leq \alpha_{\mathrm{mts}}(X,d) \leq O((\log n)^2)\,.
\]
</p>

<p>Perhaps the central remaining open question for MTS on general metric spaces
is whether the upper bound can be improved to $O(\log n)$.</p>

<h2 id="ultrametrics-from-partitions">Ultrametrics from partitions</h2>

<p>Consider a partition $P$ of $X$.  Say that $P$ is <em>$\Delta$-bounded</em>
if $S \in P \implies \mathrm{diam}_X(S) \leq \Delta$.
For a point $x \in X$, we write $P(x)$ for the unique set in $P$ that contains $x$.</p>

<p>Suppose now that $\mathcal{P} = \{ P_j : j \in \mathbb{Z} \}$ is a sequence of partitions of $X$
such that $P_j$ is $8^j$-bounded for every $j \in \mathbb{Z}$, and define the metric</p>
<p>
\[
   D_{\mathcal{P}}(x,y) \mathrel{\vcenter{:}}= \max \left\{ 8^{j+1} : P_j(x) \neq P_j(y) \right\}.
\]
</p>
<p>One can check that $D_{\mathcal{P}}$ is an ultrametric on $X$ (imagine
the tree structure induced by the partitions), and moreover</p>
<p>
\begin{equation}\label{eq:exp}
   D_{\mathcal{P}}(x,y) \geq d(x,y) \qquad \forall x,y \in X\,.
\end{equation}
</p>
<p>This follows from:  $D_{\mathcal{P}}(x,y) \leq 8^j \implies P_j(x)=P_j(y) \implies d(x,y) \leq 8^j$.</p>

<p>Define $B(x,r) \mathrel{\vcenter{:}}= \{ y \in X : d(x,y) \leq r \}$ to be the ball of radius $r$ about $x \in X$.</p>

<p class="lemma" text="Random partition lemma">
   Suppose $(X,d)$ is an $n$-point metric space.  Then for every $\Delta &gt; 0$,
   there is a random $\Delta$-bounded partition $P$ of $X$ such that for every $r \leq \Delta/8$:
   <p>
   \begin{equation}\label{eq:rp}
      \mathbb{P}\left[\vphantom{\bigoplus}
         B_X(x,r) \subseteq P(x)\right] \geq \exp\left(\frac{-8r}{\Delta} \log \frac{|B(x,\Delta)|}{|B(x,\Delta/8)|}\right).
   \end{equation}
   </p>
</p>

<p>Remarkably, the random partitioning lemma can be used to prove both <strong>Theorem 1</strong> and <strong>Theorem 2</strong>.
We will first establish these consequences and then prove the lemma.</p>

<h2 id="approximation-by-a-random-ultrametric">Approximation by a random ultrametric</h2>

<p>Let’s prove <strong><a href="#thm1">Theorem 1</a></strong>.
Let $\mathcal{P} = \{ P_j : j \in \mathbb{Z} \}$ be the random sequence where $P_j$ results from the
random partitioning lemma applied with $\Delta = 8^j$.
Then from \eqref{eq:exp}, we know $\mathbf{D}_{\mathcal{P}}(x,y) \geq d(x,y)$ for all $x,y \in X$.
Now fix $x \neq y \in X$, and let $j_0 \mathrel{\vcenter{:}}= \min \{ j : 8^{j} \geq d(x,y) \}$.  Then:</p>
<p>
\[
   \mathbb{E}\left[\mathbf{D}_{\mathcal{P}}(x,y)\right] \leq 8^{j_0} + \sum_{j &gt; j_0} \mathbb{P}[P_j(x) \neq P_j(y)] 8^{j+1},
\]
</p>
<p>and using $\mathbb{P}[P_j(x) = P_j(y)] \geq \mathbb{P}[B(x, d(x,y)) \subseteq P_j(x)]$ yields</p>
<p>
\begin{align*}
\sum_{j &gt; j_0} \mathbb{P}[P_j(x) \neq P_j(y)] 8^{j+1}
&amp;\leq \sum_{j \in \mathbb{Z}} 8^{j+2} \frac{d(x,y)}{8^{j}} \log \frac{|B(x, 8^j)|}{|B(x,8^{j-1})|} \\
&amp;\leq 64\, d(x,y) \sum_{j \in \mathbb{Z}} \log \frac{|B(x, 8^j)|}{|B(x,8^{j-1}|} \\
&amp;= 64 \,d(x,y) \log n\,.
\end{align*}
</p>
<p>where the second inequality follows from \eqref{eq:rp} and the fact that $e^{-x} \geq 1-x$,
and in the last inequality we evaluate a telescoping sum.</p>

<h2 id="finding-a-large-approximate-ultrametric-inside-x">Finding a large approximate ultrametric inside $X$</h2>

<p>Now we prove <strong><a href="#thm2">Theorem 2</a></strong>.
Let $\mathcal{P}$ be the same random partition sequence chosen above and fix some $0 &lt; \e &lt; 1/8$.
Define the random subset:</p>
<p>
\[
   \mathbf{S} \mathrel{\vcenter{:}}= \left\{ x \in X : B(x, \e 8^j) \subseteq P_j(x) \ \forall j \in \mathbb{Z}\right\}\,.
\]
</p>
<p>We claim that</p>
<p>
\[
   D_{\mathcal{P}}(x,y) \geq d(x,y) \geq \frac{\e}{8} D_{\mathcal{P}}(x,y) \qquad \forall x,y \in \mathbf{S}\,.
\]
</p>
<p>The LHS is simply from \eqref{eq:exp}.
For the RHS, observe that if $D_{\mathcal{P}}(x,y)=8^{j+1}$, then $P_j(x) \neq P_j(y)$,
hence $B(x,\e 8^j) \subseteq P_j(x) \implies d(x,y) \geq \e 8^j$.</p>

<p>Now the random partitioning lemma gives, for any $x \in X$,</p>
<p>
\[
   \mathbb{P}[x \in \mathbf{S}] \geq \prod_{j \in \mathbb{Z}} \exp\left(- 8\e \log \frac{|B(x,8^j)|}{|B(x,8^{j-1})|}\right)
   = \exp\left(-8 \e \log n\right) = n^{-8\e}\,.
\]
</p>
<p>By linearity of expectation: $\mathbb{E}[|\mathbf{S}|] \geq n^{1-8\e}$.  Taking $\e \mathrel{\vcenter{:}}= 1/16$ completes the proof.</p>

<h2 id="proof-of-the-random-partitioning-lemma">Proof of the random partitioning lemma</h2>

<p>Let $\mathbf{R} \in [\Delta/4,\Delta/2]$ be chosen uniformly,
and let $X = \{x_1, x_2, \ldots, x_n\}$ be a uniformly random
ordering of the points in $X$.
Our random partitioning is formed by iteratively carving out balls:</p>
<p>
\begin{equation}\label{eq:ckr}
   P \mathrel{\vcenter{:}}= \left\{ B(x_i, \mathbf{R}) \setminus \bigcup_{j &lt; i} B(x_j, \mathbf{R}) : i=1,2,\ldots,n\right\}.
\end{equation}
</p>
<p>Clearly $P$ is $\Delta$-bounded by construction.</p>

<p>Fix $r \leq \Delta/8$ and observe first that</p>
<p>
\[
   \mathbb{P}\left[B(x,r) \subseteq P(x) \mid \mathbf{R}\right] = \frac{|B(x,\mathbf{R}-r)|}{|B(x,\mathbf{R}+r)|}.
\]
</p>
<p>This follows because if we condition on $\mathbf{R}$, then the first center chosen in $B(x,\mathbf{R}+r)$
will decide the fate of $B(x,r)$, and the corresponding cluster will contain
the entire ball only if the point lies in $B(x,\mathbf{R}-r)$.</p>

<p>Thus we have:</p>
<p>
\begin{align*}
   \mathbb{P}\left[B(x,r) \subseteq P(x)\right] &amp;\geq \mathbb{E}\left[\frac{|B(x,\mathbf{R}-r)|}{|B(x,\mathbf{R}+r)|}\right] \\
                                         &amp;= \mathbb{E}\left[\exp\left(- \log \frac{|B(x,\mathbf{R}+r)|}{|B(x,\mathbf{R}-r)|}\right)\right]  \\
                                         &amp;\geq 
   \exp \left(\mathbb{E}\left[- \log \frac{|B(x,\mathbf{R}+r)|}{|B(x,\mathbf{R}-r)|}\right]\right)  \\
&amp;\geq \exp\left(\frac{- 8 r}{\Delta} \log \frac{|B(x,\Delta)|}{|B(x,\Delta/8)|}\right)\,,
\end{align*}
</p>
<p>where the second inequality uses convexity of $e^{-x}$ and
the last inequality comes from the calculation</p>
<p>
\begin{align*}
\mathbb{E}\left[ \log \frac{|B(x,\mathbf{R}+r)|}{|B(x,\mathbf{R}-r)|}\right] 
&amp;= \frac{4}{\Delta} \int_{\Delta/4}^{\Delta/2} 
\log \frac{|B(x,R+r)|}{|B(x,R-r)|}\,dR \\
&amp;\leq \frac{8r}{\Delta} \log \frac{|B(x,\Delta/2+r)|}{|B(x,\Delta/4-r)|} \\
&amp;\leq \frac{8r}{\Delta} \log \frac{|B(x,\Delta)|}{|B(x,\Delta/8)|}\,,
\end{align*}
</p>
<p>using $r \leq \Delta/8$.</p>

<h2 id="historical-remarks">Historical remarks</h2>

<p>The random embedding theorem (<a href="#thm1">Theorem 1</a>)
is due to <a href="https://www.sciencedirect.com/science/article/pii/S0022000004000637">Fakcharoenphol, Rao, and Talwar</a>.
The first such result was <a href="http://ieeexplore.ieee.org/iel3/4141/11790/00548477.pdf">proved by Bartal</a>, and he later
<a href="https://dl.acm.org/citation.cfm?id=276725">obtained a near-optimal bound</a>
of $O(\log n \log \log n)$ on the distortion.
The use of random tree approximations of arbitrary metric spaces
for online algorithms arose somewhat earlier in
the work of <a href="http://www.icsi.berkeley.edu/ftp/pub/techreports/1991/tr-91-066.pdf">Alon, Karp, Peleg, and West</a>,
specifically in relation to the $k$-server problem.</p>

<p>The metric Ramsey theorem (<a href="#thm2">Theorem 2</a>)
is a result of <a href="https://arxiv.org/abs/math/0406353">Bartal, Linial, Mendel, and Naor</a>,
improving over an earlier bound of <a href="https://arxiv.org/abs/cs/0406028">Bartal, Bollobas, and Mendel</a>
who established that one can take $|X’| \geq \exp(c \sqrt{\log n})$ for some $c &gt; 0$.</p>

<p>The upper bound in <a href="#thm3">Theorem 3</a>
is from a forthcoming paper with Bubeck, Cohen, and Y. T. Lee,
and improves the
$O(\log n \log \log n)$ upper bound of <a href="https://arxiv.org/abs/cs/0406034">Fiat and Mendel</a>
to the optimal value (up to a constant factor).
The lower bound in <a href="#thm3">Theorem 3</a>
is from the aforementioned paper of Bartal, Bollobas, and Mendel;
we will discuss it in the next lecture.</p>

<p>The random partitioning lemma
and the proof of <a href="#thm2">Theorem 2</a>
presented here come from a paper of <a href="https://arxiv.org/abs/cs/0511084">Mendel and Naor</a>.
This proof of the random partitioning lemma is somewhat
cleaner than the one presented there.
The (now famous) distribution on random partitions 
described in \eqref{eq:ckr} is from <a href="https://epubs.siam.org/doi/abs/10.1137/S0097539701395978">Calinescu, Karloff, and Rabani</a>.</p>


</article>






  <div id="disqus_thread"></div>
  <script type="text/javascript">
    var disqus_shortname  = 'tcsmath';
    var disqus_identifier = '/online/2018/04/12/metric-approx';
    var disqus_title      = "Approximation by ultrametrics";

    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>






      </div>
    </div>
  </div>

  
</body>
</html>
